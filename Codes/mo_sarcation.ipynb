{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZQU-d0knGyhl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import warnings\n",
        "import logging\n",
        "import gc\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "from typing import Optional\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import sklearn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import jaccard_score, f1_score, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r_Ovlo42G4Kh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "# from rouge_score.rouge_scorer import RougeScorer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from transformers import (\n",
        "    BartTokenizerFast,\n",
        "    AdamW\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    print(\"Using GPU\")\n",
        "\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "foldNum = 0\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "SOURCE_MAX_LEN = 500\n",
        "# TARGET_MAX_LEN = 50\n",
        "# MAX_UTTERANCES = 25\n",
        "\n",
        "ACOUSTIC_DIM = 768\n",
        "ACOUSTIC_MAX_LEN = 1000\n",
        "\n",
        "\n",
        "\n",
        "VISUAL_DIM = 2048\n",
        "VISUAL_MAX_LEN = 480\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "\n",
        "VALID_LEN = 69\n",
        "\n",
        "# BASE_LEARNING_RATE = 5e-6\n",
        "# NEW_LEARNING_RATE = 5e-5\n",
        "# WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# NUM_BEAMS = 5\n",
        "# EARLY_STOPPING = True\n",
        "# NO_REPEAT_NGRAM_SIZE = 3\n",
        "\n",
        "# EARLY_STOPPING_THRESHOLD = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cMdRaTfAHF7Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed : 994\n"
          ]
        }
      ],
      "source": [
        "def set_random_seed(seed: int):\n",
        "    print(\"Seed : {}\".format(seed))\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# set_random_seed(42)\n",
        "# a = np.random.randint(0, 1000)\n",
        "\n",
        "# set_random_seed(123)\n",
        "set_random_seed(994)\n",
        "# set_random_seed(12345)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
        "\n",
        "from transformers.modeling_utils import PreTrainedModel, unwrap_model\n",
        "\n",
        "from transformers import (\n",
        "    BartTokenizer,\n",
        "    AdamW\n",
        ")\n",
        "import transformers\n",
        "from transformers.models.bart.configuration_bart import BartConfig\n",
        "\n",
        "from transformers.models.bart.modeling_bart import (\n",
        "    BartPretrainedModel,\n",
        "    BartDecoder,\n",
        "    BartLearnedPositionalEmbedding,\n",
        "    BartEncoderLayer,\n",
        "    shift_tokens_right,\n",
        ")\n",
        "\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutput,\n",
        "    Seq2SeqLMOutput,\n",
        "    Seq2SeqModelOutput,\n",
        "    Seq2SeqSequenceClassifierOutput\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-ML9b3x4axdj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e1XR3BlgHOY8"
      },
      "outputs": [],
      "source": [
        "BART_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
        "            it.\n",
        "            Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "            [What are input IDs?](../glossary#input-ids)\n",
        "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "            [What are attention masks?](../glossary#attention-mask)\n",
        "        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
        "            Indices of decoder input sequence tokens in the vocabulary.\n",
        "            Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "            [What are decoder input IDs?](../glossary#decoder-input-ids)\n",
        "            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n",
        "            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n",
        "            For translation and summarization training, `decoder_input_ids` should be provided. If no\n",
        "            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n",
        "            for denoising pre-training following the paper.\n",
        "        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
        "            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n",
        "            be used by default.\n",
        "            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n",
        "            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n",
        "            information on the default strategy.\n",
        "        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
        "            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n",
        "            1]`:\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n",
        "            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n",
        "            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n",
        "            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n",
        "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
        "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
        "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
        "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
        "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
        "            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
        "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
        "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
        "            `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of shape\n",
        "            `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing `input_ids` you\n",
        "            can choose to directly pass an embedded representation. This is useful if you want more control over how to\n",
        "            convert `input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n",
        "        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n",
        "            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n",
        "            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n",
        "            input (see `past_key_values`). This is useful if you want more control over how to convert\n",
        "            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n",
        "            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n",
        "            of `inputs_embeds`.\n",
        "        use_cache (`bool`, *optional*):\n",
        "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
        "            `past_key_values`).\n",
        "        output_attentions (`bool`, *optional*):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (`bool`, *optional*):\n",
        "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (`bool`, *optional*):\n",
        "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aEDR_7N6HWsu"
      },
      "outputs": [],
      "source": [
        "class ContextAwareAttention(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 dim_model : int,\n",
        "                 dim_context : int,\n",
        "                 dropout_rate : Optional[float] = 0.0 ):\n",
        "\n",
        "        super(ContextAwareAttention, self).__init__()\n",
        "\n",
        "        self.dim_model = dim_model\n",
        "        self.dim_context = dim_context\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.attention_layer = nn.MultiheadAttention(embed_dim=self.dim_model,\n",
        "                                                     num_heads = 1,\n",
        "                                                     dropout = self.dropout_rate,\n",
        "                                                     bias = True,\n",
        "                                                    add_zero_attn=False,\n",
        "                                                    batch_first=True,\n",
        "                                                    device=DEVICE\n",
        "        )\n",
        "\n",
        "        self.u_k = nn.Linear(self.dim_context, self.dim_model, bias = False)\n",
        "        self.w1_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "        self.w2_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "\n",
        "        self.u_v = nn.Linear(self.dim_context, self.dim_model, bias=False)\n",
        "        self.w1_v = nn.Linear(self.dim_model, 1, bias = False)\n",
        "        self.w2_v = nn.Linear(self.dim_model, 1, bias = False)\n",
        "\n",
        "    def forward(self, q, k, v, context):\n",
        "\n",
        "        # print(\"Context shape : \", context.shape)\n",
        "        # print(\"Dim context : \", self.dim_context, \" : Dim model : \", self.dim_model)\n",
        "        key_context = self.u_k(context)\n",
        "        # print(\"Context shape below key context : \", key_context.shape)\n",
        "        value_context = self.u_v(context)\n",
        "\n",
        "        lambda_k = F.sigmoid(self.w1_k(k) + self.w2_k(key_context))\n",
        "        lambda_v = F.sigmoid(self.w1_v(v) + self.w2_v(value_context))\n",
        "\n",
        "        k_cap = (1-lambda_k) * k + (lambda_k) * key_context\n",
        "        v_cap = (1-lambda_v) * v + (lambda_v) * value_context\n",
        "\n",
        "        attention_output, _ = self.attention_layer(query = q,\n",
        "                                                   key = k_cap,\n",
        "                                                   value = v_cap)\n",
        "\n",
        "        return attention_output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2ZPn4U3Weq9I"
      },
      "outputs": [],
      "source": [
        "class MAF_acoustic(nn.Module):\n",
        "    def __init__(self,\n",
        "                dim_model,\n",
        "                dropout_rate):\n",
        "        super(MAF_acoustic, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "        # self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "        self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "                                                                dim_context=ACOUSTIC_DIM,\n",
        "                                                                dropout_rate=dropout_rate)\n",
        "\n",
        "        # self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "        #                                                     dim_context=VISUAL_DIM,\n",
        "        #                                                     dropout_rate=dropout_rate)\n",
        "\n",
        "        self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n",
        "        # self.visual_gate = nn.Linear(2*dim_model, dim_model)\n",
        "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "        self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "    def forward(self,\n",
        "                text_input,\n",
        "                acoustic_context):\n",
        "\n",
        "        # print(\"Acoustic context shape (A) : \", acoustic_context.shape)\n",
        "\n",
        "        acoustic_context = acoustic_context.permute(0,2,1)\n",
        "        acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n",
        "        acoustic_context = acoustic_context.permute(0,2,1)\n",
        "\n",
        "        audio_out = self.acoustic_context_attention(q=text_input,\n",
        "                                                    k=text_input,\n",
        "                                                    v=text_input,\n",
        "                                                    context=acoustic_context)\n",
        "        # print(\"Audio out (A) : \", audio_out.shape)\n",
        "\n",
        "        # print(\"Visual context shape : \", visual_context.shape)\n",
        "        # visual_context = visual_context.permute(0,2,1)\n",
        "        # visual_context = self.visual_context_transform(visual_context.float())\n",
        "        # visual_context = visual_context.permute(0,2,1)\n",
        "\n",
        "        # video_out = self.visual_context_attention(q=text_input,\n",
        "        #                                             k=text_input,\n",
        "        #                                             v=text_input,\n",
        "        #                                             context=visual_context)\n",
        "\n",
        "        # print(\"Video out shape : \", video_out.shape)\n",
        "        # print(\"Text input shape : \", text_input.shape)\n",
        "        weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n",
        "        # weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n",
        "\n",
        "        # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n",
        "\n",
        "        output = self.final_layer_norm(text_input + weight_a * audio_out)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZeZdIlcker1V"
      },
      "outputs": [],
      "source": [
        "class MAF_visual(nn.Module):\n",
        "    def __init__(self,\n",
        "                dim_model,\n",
        "                dropout_rate):\n",
        "        super(MAF_visual, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "        self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "        # self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "        #                                                         dim_context=ACOUSTIC_DIM,\n",
        "        #                                                         dropout_rate=dropout_rate)\n",
        "\n",
        "        self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "                                                            dim_context=VISUAL_DIM,\n",
        "                                                            dropout_rate=dropout_rate)\n",
        "\n",
        "        # self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n",
        "        self.visual_gate = nn.Linear(2*dim_model, dim_model)\n",
        "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "        self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "    def forward(self,\n",
        "                text_input,\n",
        "                visual_context):\n",
        "\n",
        "        # print(\"Acoustic context shape (A) : \", acoustic_context.shape)\n",
        "\n",
        "        # acoustic_context = acoustic_context.permute(0,2,1)\n",
        "        # acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n",
        "        # acoustic_context = acoustic_context.permute(0,2,1)\n",
        "\n",
        "        # audio_out = self.acoustic_context_attention(q=text_input,\n",
        "        #                                             k=text_input,\n",
        "        #                                             v=text_input,\n",
        "        #                                             context=acoustic_context)\n",
        "        # print(\"Audio out (A) : \", audio_out.shape)\n",
        "\n",
        "        # print(\"Visual context shape : \", visual_context.shape)\n",
        "        visual_context = visual_context.permute(0,2,1)\n",
        "        visual_context = self.visual_context_transform(visual_context.float())\n",
        "        visual_context = visual_context.permute(0,2,1)\n",
        "\n",
        "        video_out = self.visual_context_attention(q=text_input,\n",
        "                                                    k=text_input,\n",
        "                                                    v=text_input,\n",
        "                                                    context=visual_context)\n",
        "\n",
        "        # print(\"Video out shape : \", video_out.shape)\n",
        "        # print(\"Text input shape : \", text_input.shape)\n",
        "        # weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n",
        "        weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n",
        "\n",
        "        # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n",
        "\n",
        "        output = self.final_layer_norm(text_input  + weight_v * video_out)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "s88m19VoHbj5"
      },
      "outputs": [],
      "source": [
        "class MultiModalBartEncoder(BartPretrainedModel):\n",
        "\n",
        "    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.encoder_layerdrop\n",
        "\n",
        "        embed_dim = config.d_model\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_source_position = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n",
        "\n",
        "        self.embed_positions = BartLearnedPositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            embed_dim\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.layers = nn.ModuleList([BartEncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "\n",
        "        self.layernorm_embedding = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.init_weights()\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "        # self.fusion_at_layer = [4]\n",
        "        # self.fusion_at_layer = [3, 4]\n",
        "        # self.fusion_at_layer3 = [3]\n",
        "        self.fusion_at_layer4 = [4]\n",
        "        self.fusion_at_layer5 = [5]\n",
        "\n",
        "        # self.fusion_of_context = [3]\n",
        "        # self.visual_transformer = TransformerEncoder(d_model = VISUAL_DIM,\n",
        "        #                                              n_layers = 4,\n",
        "        #                                              n_heads=8,\n",
        "        #                                              d_ff=VISUAL_DIM\n",
        "        #                                              )\n",
        "        # self.acoustic_transformer = TransformerEncoder(d_model = ACOUSTIC_DIM,\n",
        "        #                                                n_layers=4,\n",
        "        #                                                n_heads=2,\n",
        "        #                                                d_ff=ACOUSTIC_DIM)\n",
        "\n",
        "        # self.MAF_layer3 = MAF(dim_model=embed_dim,\n",
        "        #                      dropout_rate=0.2)\n",
        "\n",
        "        self.MAF_layer4 = MAF_acoustic(dim_model=embed_dim,\n",
        "                             dropout_rate=0.2)\n",
        "\n",
        "        self.MAF_layer5 = MAF_visual(dim_model=embed_dim,\n",
        "                             dropout_rate=0.2)\n",
        "\n",
        "        # self.context_encoder = ContextEncoder(config)\n",
        "\n",
        "        # self.classification = nn.Linear(embed_dim, 2)\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "            input_ids = None,\n",
        "            attention_mask = None,\n",
        "            # context_input_ids = None,\n",
        "            # context_attention_mask = None,\n",
        "            acoustic_input = None,\n",
        "            visual_input = None,\n",
        "            head_mask = None,\n",
        "            inputs_embeds = None,\n",
        "            output_attentions = None,\n",
        "            output_hidden_states  = None,\n",
        "            return_dict = None):\n",
        "\n",
        "            # print(\"Input ids shape : \", input_ids.shape)\n",
        "            output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "\n",
        "            output_hidden_states = (\n",
        "                output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "            )\n",
        "\n",
        "            return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "            if input_ids is not None and inputs_embeds is not None:\n",
        "                raise ValueError(\"You can't specify both input_ids and inputs_embeds at the same time\")\n",
        "            elif input_ids is not None:\n",
        "                input_shape = input_ids.size()\n",
        "                input_ids = input_ids.view(-1, input_shape[-1])\n",
        "\n",
        "            elif inputs_embeds is not None:\n",
        "                input_shape = inputs_embeds.size()[:-1]\n",
        "            else:\n",
        "                raise ValueError(\"You have to specify either input_ids or input_embeds\")\n",
        "\n",
        "\n",
        "            if inputs_embeds is None:\n",
        "                # print(\"Input ids shape : \", input_ids.shape)\n",
        "                inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "            # print(\"Input shape type : \", type(input_shape))\n",
        "            # print(\"Input shape : \", input_shape)\n",
        "            input_shape = torch.tensor(input_shape)\n",
        "            # print(\"Input shape type : \", type(input_shape))\n",
        "            # print(\"Input shape : \", input_shape)\n",
        "            # print(\"Input shape : \", input_shape.shape)\n",
        "            embed_pos = self.embed_positions(input_ids)\n",
        "            # embed_pos = self.embed_positions(input_shape)\n",
        "\n",
        "\n",
        "            hidden_states = inputs_embeds + embed_pos\n",
        "            hidden_states = self.layernorm_embedding(hidden_states)\n",
        "            hidden_states = F.dropout(hidden_states, p = self.dropout, training=self.training)\n",
        "\n",
        "            # print(\"attention mask shape 3 : \", attention_mask.shape)\n",
        "            if attention_mask is not None:\n",
        "                attention_mask =  _expand_mask(attention_mask, inputs_embeds.dtype)\n",
        "\n",
        "            # print(\"attention mask shape 4 : \", attention_mask.shape)\n",
        "            encoder_states = () if output_hidden_states else None\n",
        "            all_attentions = () if output_attentions else None\n",
        "\n",
        "            if head_mask is not None:\n",
        "                assert head_mask.size()[0] == (\n",
        "                    len(self.layers)\n",
        "                ), f\"The head mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
        "\n",
        "            for idx, encoder_layer in enumerate(self.layers):\n",
        "                # print(\"============Idx : \", idx)\n",
        "\n",
        "                # if idx in self.fusion_at_layer3:\n",
        "                #     # print(\"Acoustic input shape (B) : \", acoustic_input)\n",
        "                #     # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]\n",
        "                #     # print(\"Acoustic input shape (C) : \", acoustic_input)\n",
        "\n",
        "                #     # visual_input = self.visual_transformer(visual_input)[-1]\n",
        "                #     # print(\"====Idx inside fusion at layer :\", idx)\n",
        "                #     hidden_states = self.MAF_layer3(text_input = hidden_states,\n",
        "                #                                    acoustic_context = acoustic_input,\n",
        "                #                                    visual_context = visual_input)\n",
        "\n",
        "                # if idx in self.fusion_of_context:\n",
        "\n",
        "                #   hidden_states = self.context_encoder(hidden_states = hidden_states, context_input_ids = context_input_ids, context_attention_mask = context_attention_mask)\n",
        "\n",
        "\n",
        "                if idx in self.fusion_at_layer4:\n",
        "                    # print(\"Acoustic input shape (B) : \", acoustic_input)\n",
        "                    # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]\n",
        "                    # print(\"Acoustic input shape (C) : \", acoustic_input)\n",
        "\n",
        "                    # visual_input = self.visual_transformer(visual_input)[-1]\n",
        "                    # print(\"====Idx inside fusion at layer :\", idx)\n",
        "\n",
        "                    hidden_states = self.MAF_layer4(text_input = hidden_states,\n",
        "                                                   acoustic_context = acoustic_input\n",
        "                                                   )\n",
        "                if idx in self.fusion_at_layer5:\n",
        "                    # print(\"Acoustic input shape (B) : \", acoustic_input)\n",
        "                    # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]\n",
        "                    # print(\"Acoustic input shape (C) : \", acoustic_input)\n",
        "\n",
        "                    # visual_input = self.visual_transformer(visual_input)[-1]\n",
        "                    # print(\"====Idx inside fusion at layer :\", idx)\n",
        "                    hidden_states = self.MAF_layer5(text_input = hidden_states,\n",
        "                                                   visual_context = visual_input)\n",
        "\n",
        "                if output_hidden_states:\n",
        "                    encoder_states = encoder_states + (hidden_states,)\n",
        "\n",
        "                dropout_probability = random.uniform(0,1)\n",
        "\n",
        "                if self.training and (dropout_probability < self.layerdrop):\n",
        "                    layer_outputs = (None, None)\n",
        "\n",
        "                else:\n",
        "                    if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                        def create_custom_forward(module):\n",
        "                            def custom_forward(*inputs):\n",
        "                                return module(*inputs, output_attentions)\n",
        "\n",
        "                            return custom_forward\n",
        "\n",
        "                        layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                            create_custom_forward(encoder_layer),\n",
        "                            hidden_states,\n",
        "                            attention_mask,\n",
        "                            (head_mask[idx] if head_mask is not None else None),\n",
        "                        )\n",
        "\n",
        "                    else:\n",
        "                        # print(\"Checking Attention mask shape : \", attention_mask.shape)\n",
        "                        layer_outputs = encoder_layer(\n",
        "                            hidden_states,\n",
        "                            attention_mask,\n",
        "                            layer_head_mask = (head_mask[idx] if head_mask is not None else None),\n",
        "                            output_attentions = output_attentions\n",
        "                        )\n",
        "\n",
        "                    hidden_states = layer_outputs[0]\n",
        "\n",
        "                if output_attentions:\n",
        "                    all_attentions  = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "            if output_hidden_states:\n",
        "                encoder_states = encoder_states + (hidden_states,)\n",
        "\n",
        "            if not return_dict:\n",
        "                return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
        "\n",
        "            return BaseModelOutput(\n",
        "                last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
        "            )\n",
        "\n",
        "            # print(\"Hidden states shape : \", hidden_states)\n",
        "\n",
        "            # cls = hidden_states.permute(1,0,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6F_nfly0HgKR"
      },
      "outputs": [],
      "source": [
        "class MultimodalBartModel(BartPretrainedModel):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
        "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
        "\n",
        "        self.encoder = MultiModalBartEncoder(config, self.shared)\n",
        "        self.decoder = BartDecoder(config, self.shared)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.shared = value\n",
        "        self.encoder.embed_tokens = self.shared\n",
        "        self.decoder.embed_tokens = self.shared\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.encoder\n",
        "\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.decoder\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids = None,\n",
        "        attention_mask = None,\n",
        "        # context_input_ids = None,\n",
        "        # context_attention_mask = None,\n",
        "        acoustic_input = None,\n",
        "        visual_input = None,\n",
        "        decoder_input_ids = None,\n",
        "        decoder_attention_mask = None,\n",
        "        head_mask = None,\n",
        "        decoder_head_mask = None,\n",
        "        cross_attn_head_mask = None,\n",
        "        encoder_outputs = None,\n",
        "        past_key_values = None,\n",
        "        inputs_embeds = None,\n",
        "        decoder_inputs_embeds = None,\n",
        "        use_cache = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None\n",
        "    ):\n",
        "\n",
        "        if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
        "            decoder_input_ids = shift_tokens_right(\n",
        "                input_ids, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "\n",
        "            )\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # print(\"attention mask shape 2 : \", attention_mask.shape)\n",
        "\n",
        "        if encoder_outputs is None:\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids = input_ids,\n",
        "                attention_mask = attention_mask,\n",
        "                # context_input_ids = context_input_ids,\n",
        "                # context_attention_mask = context_attention_mask,\n",
        "                acoustic_input = acoustic_input,\n",
        "                visual_input = visual_input,\n",
        "                head_mask = head_mask,\n",
        "                inputs_embeds = inputs_embeds,\n",
        "                output_attentions = output_attentions,\n",
        "                output_hidden_states = output_hidden_states,\n",
        "                return_dict = return_dict\n",
        "            )\n",
        "\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            encoder_outputs = BaseModelOutput(\n",
        "                last_hidden_state=encoder_outputs[0],\n",
        "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "                attentions = encoder_outputs[2] if len(encoder_outputs) > 2 else None\n",
        "            )\n",
        "\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids = decoder_input_ids,\n",
        "            attention_mask = decoder_attention_mask,\n",
        "            encoder_hidden_states = encoder_outputs[0],\n",
        "            encoder_attention_mask = attention_mask,\n",
        "            head_mask = decoder_head_mask,\n",
        "            cross_attn_head_mask = cross_attn_head_mask,\n",
        "            past_key_values = past_key_values,\n",
        "            inputs_embeds = decoder_inputs_embeds,\n",
        "            use_cache = use_cache,\n",
        "            output_attentions = output_attentions,\n",
        "            output_hidden_states = output_hidden_states,\n",
        "            return_dict = return_dict\n",
        "        )\n",
        "\n",
        "        if not return_dict:\n",
        "            return decoder_outputs + encoder_outputs\n",
        "\n",
        "        return Seq2SeqModelOutput(\n",
        "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "            past_key_values=decoder_outputs.past_key_values,\n",
        "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "            decoder_attentions=decoder_outputs.attentions,\n",
        "            cross_attentions=decoder_outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "            encoder_attentions=encoder_outputs.attentions\n",
        "\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VO1KdKseHlkn"
      },
      "outputs": [],
      "source": [
        "class MultimodalBartClassification(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        inned_dim: int,\n",
        "        num_classes: int,\n",
        "        pooler_dropout: float\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, inned_dim)\n",
        "        self.dropout = nn.Dropout(p = pooler_dropout)\n",
        "        self.out_proj = nn.Linear(inned_dim, num_classes)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = torch.tanh(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.out_proj(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultimodalBartForSequenceClassification(BartPretrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n",
        "\n",
        "    def __init__(self, config: BartConfig, **kwargs):\n",
        "        super().__init__(config, **kwargs)\n",
        "        self.model = MultimodalBartModel(config)\n",
        "        self.classification_head = MultimodalBartClassification(\n",
        "            config.d_model,\n",
        "            config.d_model,\n",
        "            2,\n",
        "            config.classifier_dropout\n",
        "        )\n",
        "        self.model._init_weights(self.classification_head.dense)\n",
        "        self.model._init_weights(self.classification_head.out_proj)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.tensor] = None,\n",
        "        # context_input_ids : torch.LongTensor = None,\n",
        "        # context_attention_mask : Optional[torch.tensor] = None,\n",
        "        acoustic_input = None,\n",
        "        visual_input = None,\n",
        "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None\n",
        "    ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "            config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        if labels is not None:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is None and inputs_embeds is not None:\n",
        "            raise NotImplementedError(\n",
        "                f\"Passing input embeddings is currently not supported for {self.__class__.__name__}\"\n",
        "            )\n",
        "\n",
        "        # print(\"attention mask shape 1 : \", attention_mask.shape )\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask = attention_mask,\n",
        "            # context_input_ids = context_input_ids,\n",
        "            # context_attention_mask = context_attention_mask,\n",
        "            acoustic_input = acoustic_input,\n",
        "            visual_input = visual_input,\n",
        "            decoder_input_ids = decoder_input_ids,\n",
        "            decoder_attention_mask = decoder_attention_mask,\n",
        "            head_mask = head_mask,\n",
        "            decoder_head_mask = decoder_head_mask,\n",
        "            cross_attn_head_mask = cross_attn_head_mask,\n",
        "            encoder_outputs = encoder_outputs,\n",
        "            inputs_embeds = inputs_embeds,\n",
        "            decoder_inputs_embeds = decoder_inputs_embeds,\n",
        "            use_cache = use_cache,\n",
        "            output_attentions = output_attentions,\n",
        "            output_hidden_states = output_hidden_states,\n",
        "            return_dict = return_dict\n",
        "\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs[0]\n",
        "\n",
        "        eos_mask = input_ids.eq(self.config.eos_token_id).to(hidden_states.device)\n",
        "\n",
        "        if len(torch.unique_consecutive(eos_mask.sum(1))) > 1:\n",
        "            raise ValueError(\"All examples must have same number of <eos> tokens\")\n",
        "\n",
        "        sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[\n",
        "            :, -1, :\n",
        "        ]\n",
        "\n",
        "        logits = self.classification_head(sentence_representation)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        # print(\"logits shape : \", logits.shape)\n",
        "        loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "        # print(\"Loss : \", loss)\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "\n",
        "        return Seq2SeqSequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
        "            decoder_attentions = outputs.decoder_attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
        "            encoder_attentions=outputs.encoder_attentions\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "FOLDER_NAME = '/backup/hatemm/Dataset/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KaewwlapryS5"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/Colab Notebooks/32/train_dlog_id.p', 'rb') as f:\n",
        "#   train_dlog_id = pickle.load(f)\n",
        "\n",
        "with open(FOLDER_NAME + 'all__video_vosk_audioMap.pkl', 'rb') as f:\n",
        "  transcript = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9etiInqr4-9"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/Colab Notebooks/32/test_dlog_id.p', 'rb') as f:\n",
        "#   test_dlog_id = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RunFr5b0Ni1u"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/Colab Notebooks/32/train_audio_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "#   train_audio_data_utterance1 = pickle.load(f)\n",
        "\n",
        "with open(FOLDER_NAME + 'Wav2Vec2_features_chunked.pkl', 'rb') as fo:\n",
        "  audio_data = pickle.load(fo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Yw0sc0aaNzOO"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/Colab Notebooks/32/train_video_fold_'+str(foldNum)+'.p', 'rb') as f:\n",
        "#   train_image_data_utterance1 = pickle.load(f)\n",
        "\n",
        "with open(FOLDER_NAME + 'noFoldDetails.pkl', 'rb') as fp:\n",
        "    video_labels = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qO9elV8EQAi1"
      },
      "outputs": [],
      "source": [
        "def pad_seq(tensor, dim, max_len):\n",
        "  if max_len > tensor.shape[0] :\n",
        "    return torch.cat([tensor, torch.zeros(max_len - tensor.shape[0], dim)])\n",
        "  else:\n",
        "    return tensor[:max_len]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cUPe2LxQ04S"
      },
      "outputs": [],
      "source": [
        "train_audio_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = ACOUSTIC_DIM,\n",
        "                                                   max_len = ACOUSTIC_MAX_LEN)\n",
        "                                                  for a in train_audio_data_utterance1], 0)\n",
        "train_audio_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6-frFIpTzrW"
      },
      "outputs": [],
      "source": [
        "test_audio_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = ACOUSTIC_DIM,\n",
        "                                                   max_len = ACOUSTIC_MAX_LEN)\n",
        "                                                  for a in test_audio_data_utterance1], 0)\n",
        "test_audio_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO2OLh5yUTF2"
      },
      "outputs": [],
      "source": [
        "train_image_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = VISUAL_DIM,\n",
        "                                                   max_len = VISUAL_MAX_LEN)\n",
        "                                                  for a in train_image_data_utterance1], 0)\n",
        "train_image_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BAAQoCcUv34"
      },
      "outputs": [],
      "source": [
        "test_image_data_utterance1 = torch.stack([pad_seq(torch.tensor(a, dtype = torch.float),\n",
        "                                                   dim = VISUAL_DIM,\n",
        "                                                   max_len = VISUAL_MAX_LEN)\n",
        "                                                  for a in test_image_data_utterance1], 0)\n",
        "test_image_data_utterance1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTMNyPhSUdBs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pdf5hGlVH5Zw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "config.json: 100%|██████████| 1.72k/1.72k [00:00<00:00, 1.93MB/s]\n",
            "model.safetensors: 100%|██████████| 558M/558M [00:05<00:00, 109MB/s]  \n",
            "Some weights of MultimodalBartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'encoder.MAF_layer4.acoustic_context_attention.attention_layer.in_proj_bias', 'encoder.MAF_layer4.acoustic_context_attention.attention_layer.in_proj_weight', 'encoder.MAF_layer4.acoustic_context_attention.attention_layer.out_proj.bias', 'encoder.MAF_layer4.acoustic_context_attention.attention_layer.out_proj.weight', 'encoder.MAF_layer4.acoustic_context_attention.u_k.weight', 'encoder.MAF_layer4.acoustic_context_attention.u_v.weight', 'encoder.MAF_layer4.acoustic_context_attention.w1_k.weight', 'encoder.MAF_layer4.acoustic_context_attention.w1_v.weight', 'encoder.MAF_layer4.acoustic_context_attention.w2_k.weight', 'encoder.MAF_layer4.acoustic_context_attention.w2_v.weight', 'encoder.MAF_layer4.acoustic_context_transform.weight', 'encoder.MAF_layer4.acoustic_gate.bias', 'encoder.MAF_layer4.acoustic_gate.weight', 'encoder.MAF_layer4.final_layer_norm.bias', 'encoder.MAF_layer4.final_layer_norm.weight', 'encoder.MAF_layer5.final_layer_norm.bias', 'encoder.MAF_layer5.final_layer_norm.weight', 'encoder.MAF_layer5.visual_context_attention.attention_layer.in_proj_bias', 'encoder.MAF_layer5.visual_context_attention.attention_layer.in_proj_weight', 'encoder.MAF_layer5.visual_context_attention.attention_layer.out_proj.bias', 'encoder.MAF_layer5.visual_context_attention.attention_layer.out_proj.weight', 'encoder.MAF_layer5.visual_context_attention.u_k.weight', 'encoder.MAF_layer5.visual_context_attention.u_v.weight', 'encoder.MAF_layer5.visual_context_attention.w1_k.weight', 'encoder.MAF_layer5.visual_context_attention.w1_v.weight', 'encoder.MAF_layer5.visual_context_attention.w2_k.weight', 'encoder.MAF_layer5.visual_context_attention.w2_v.weight', 'encoder.MAF_layer5.visual_context_transform.weight', 'encoder.MAF_layer5.visual_gate.bias', 'encoder.MAF_layer5.visual_gate.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultimodalBartForSequenceClassification(\n",
            "  (model): MultimodalBartModel(\n",
            "    (shared): Embedding(50265, 768, padding_idx=1)\n",
            "    (encoder): MultiModalBartEncoder(\n",
            "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
            "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
            "      (layers): ModuleList(\n",
            "        (0-5): 6 x BartEncoderLayer(\n",
            "          (self_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (activation_fn): GELUActivation()\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (MAF_layer4): MAF_acoustic(\n",
            "        (acoustic_context_transform): Linear(in_features=1000, out_features=500, bias=False)\n",
            "        (acoustic_context_attention): ContextAwareAttention(\n",
            "          (attention_layer): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (u_k): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (w1_k): Linear(in_features=768, out_features=1, bias=False)\n",
            "          (w2_k): Linear(in_features=768, out_features=1, bias=False)\n",
            "          (u_v): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (w1_v): Linear(in_features=768, out_features=1, bias=False)\n",
            "          (w2_v): Linear(in_features=768, out_features=1, bias=False)\n",
            "        )\n",
            "        (acoustic_gate): Linear(in_features=1536, out_features=768, bias=True)\n",
            "        (dropout_layer): Dropout(p=0.2, inplace=False)\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (MAF_layer5): MAF_visual(\n",
            "        (visual_context_transform): Linear(in_features=480, out_features=500, bias=False)\n",
            "        (visual_context_attention): ContextAwareAttention(\n",
            "          (attention_layer): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (u_k): Linear(in_features=2048, out_features=768, bias=False)\n",
            "          (w1_k): Linear(in_features=768, out_features=1, bias=False)\n",
            "          (w2_k): Linear(in_features=768, out_features=1, bias=False)\n",
            "          (u_v): Linear(in_features=2048, out_features=768, bias=False)\n",
            "          (w1_v): Linear(in_features=768, out_features=1, bias=False)\n",
            "          (w2_v): Linear(in_features=768, out_features=1, bias=False)\n",
            "        )\n",
            "        (visual_gate): Linear(in_features=1536, out_features=768, bias=True)\n",
            "        (dropout_layer): Dropout(p=0.2, inplace=False)\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (decoder): BartDecoder(\n",
            "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
            "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
            "      (layers): ModuleList(\n",
            "        (0-5): 6 x BartDecoderLayer(\n",
            "          (self_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (activation_fn): GELUActivation()\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_head): MultimodalBartClassification(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.00MB/s]\n",
            "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.06MB/s]\n",
            "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 3.66MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer :  BartTokenizerFast(name_or_path='facebook/bart-base', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
            "}\n",
            "Total parameters :  152.172706\n"
          ]
        }
      ],
      "source": [
        "model = MultimodalBartForSequenceClassification.from_pretrained(\"facebook/bart-base\")\n",
        "print(model)\n",
        "\n",
        "tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-base')\n",
        "print(\"Tokenizer : \", tokenizer)\n",
        "\n",
        "num_param = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters : \", num_param/1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "n_N4HcMQ9hXX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count :  0  name :  model.shared.weight\n",
            "Count :  1  name :  model.encoder.embed_positions.weight\n",
            "Count :  2  name :  model.encoder.layers.0.self_attn.k_proj.weight\n",
            "Count :  3  name :  model.encoder.layers.0.self_attn.k_proj.bias\n",
            "Count :  4  name :  model.encoder.layers.0.self_attn.v_proj.weight\n",
            "Count :  5  name :  model.encoder.layers.0.self_attn.v_proj.bias\n",
            "Count :  6  name :  model.encoder.layers.0.self_attn.q_proj.weight\n",
            "Count :  7  name :  model.encoder.layers.0.self_attn.q_proj.bias\n",
            "Count :  8  name :  model.encoder.layers.0.self_attn.out_proj.weight\n",
            "Count :  9  name :  model.encoder.layers.0.self_attn.out_proj.bias\n",
            "Count :  10  name :  model.encoder.layers.0.self_attn_layer_norm.weight\n",
            "Count :  11  name :  model.encoder.layers.0.self_attn_layer_norm.bias\n",
            "Count :  12  name :  model.encoder.layers.0.fc1.weight\n",
            "Count :  13  name :  model.encoder.layers.0.fc1.bias\n",
            "Count :  14  name :  model.encoder.layers.0.fc2.weight\n",
            "Count :  15  name :  model.encoder.layers.0.fc2.bias\n",
            "Count :  16  name :  model.encoder.layers.0.final_layer_norm.weight\n",
            "Count :  17  name :  model.encoder.layers.0.final_layer_norm.bias\n",
            "Count :  18  name :  model.encoder.layers.1.self_attn.k_proj.weight\n",
            "Count :  19  name :  model.encoder.layers.1.self_attn.k_proj.bias\n",
            "Count :  20  name :  model.encoder.layers.1.self_attn.v_proj.weight\n",
            "Count :  21  name :  model.encoder.layers.1.self_attn.v_proj.bias\n",
            "Count :  22  name :  model.encoder.layers.1.self_attn.q_proj.weight\n",
            "Count :  23  name :  model.encoder.layers.1.self_attn.q_proj.bias\n",
            "Count :  24  name :  model.encoder.layers.1.self_attn.out_proj.weight\n",
            "Count :  25  name :  model.encoder.layers.1.self_attn.out_proj.bias\n",
            "Count :  26  name :  model.encoder.layers.1.self_attn_layer_norm.weight\n",
            "Count :  27  name :  model.encoder.layers.1.self_attn_layer_norm.bias\n",
            "Count :  28  name :  model.encoder.layers.1.fc1.weight\n",
            "Count :  29  name :  model.encoder.layers.1.fc1.bias\n",
            "Count :  30  name :  model.encoder.layers.1.fc2.weight\n",
            "Count :  31  name :  model.encoder.layers.1.fc2.bias\n",
            "Count :  32  name :  model.encoder.layers.1.final_layer_norm.weight\n",
            "Count :  33  name :  model.encoder.layers.1.final_layer_norm.bias\n",
            "Count :  34  name :  model.encoder.layers.2.self_attn.k_proj.weight\n",
            "Count :  35  name :  model.encoder.layers.2.self_attn.k_proj.bias\n",
            "Count :  36  name :  model.encoder.layers.2.self_attn.v_proj.weight\n",
            "Count :  37  name :  model.encoder.layers.2.self_attn.v_proj.bias\n",
            "Count :  38  name :  model.encoder.layers.2.self_attn.q_proj.weight\n",
            "Count :  39  name :  model.encoder.layers.2.self_attn.q_proj.bias\n",
            "Count :  40  name :  model.encoder.layers.2.self_attn.out_proj.weight\n",
            "Count :  41  name :  model.encoder.layers.2.self_attn.out_proj.bias\n",
            "Count :  42  name :  model.encoder.layers.2.self_attn_layer_norm.weight\n",
            "Count :  43  name :  model.encoder.layers.2.self_attn_layer_norm.bias\n",
            "Count :  44  name :  model.encoder.layers.2.fc1.weight\n",
            "Count :  45  name :  model.encoder.layers.2.fc1.bias\n",
            "Count :  46  name :  model.encoder.layers.2.fc2.weight\n",
            "Count :  47  name :  model.encoder.layers.2.fc2.bias\n",
            "Count :  48  name :  model.encoder.layers.2.final_layer_norm.weight\n",
            "Count :  49  name :  model.encoder.layers.2.final_layer_norm.bias\n",
            "Count :  50  name :  model.encoder.layers.3.self_attn.k_proj.weight\n",
            "Count :  51  name :  model.encoder.layers.3.self_attn.k_proj.bias\n",
            "Count :  52  name :  model.encoder.layers.3.self_attn.v_proj.weight\n",
            "Count :  53  name :  model.encoder.layers.3.self_attn.v_proj.bias\n",
            "Count :  54  name :  model.encoder.layers.3.self_attn.q_proj.weight\n",
            "Count :  55  name :  model.encoder.layers.3.self_attn.q_proj.bias\n",
            "Count :  56  name :  model.encoder.layers.3.self_attn.out_proj.weight\n",
            "Count :  57  name :  model.encoder.layers.3.self_attn.out_proj.bias\n",
            "Count :  58  name :  model.encoder.layers.3.self_attn_layer_norm.weight\n",
            "Count :  59  name :  model.encoder.layers.3.self_attn_layer_norm.bias\n",
            "Count :  60  name :  model.encoder.layers.3.fc1.weight\n",
            "Count :  61  name :  model.encoder.layers.3.fc1.bias\n",
            "Count :  62  name :  model.encoder.layers.3.fc2.weight\n",
            "Count :  63  name :  model.encoder.layers.3.fc2.bias\n",
            "Count :  64  name :  model.encoder.layers.3.final_layer_norm.weight\n",
            "Count :  65  name :  model.encoder.layers.3.final_layer_norm.bias\n",
            "Count :  66  name :  model.encoder.layers.4.self_attn.k_proj.weight\n",
            "Count :  67  name :  model.encoder.layers.4.self_attn.k_proj.bias\n",
            "Count :  68  name :  model.encoder.layers.4.self_attn.v_proj.weight\n",
            "Count :  69  name :  model.encoder.layers.4.self_attn.v_proj.bias\n",
            "Count :  70  name :  model.encoder.layers.4.self_attn.q_proj.weight\n",
            "Count :  71  name :  model.encoder.layers.4.self_attn.q_proj.bias\n",
            "Count :  72  name :  model.encoder.layers.4.self_attn.out_proj.weight\n",
            "Count :  73  name :  model.encoder.layers.4.self_attn.out_proj.bias\n",
            "Count :  74  name :  model.encoder.layers.4.self_attn_layer_norm.weight\n",
            "Count :  75  name :  model.encoder.layers.4.self_attn_layer_norm.bias\n",
            "Count :  76  name :  model.encoder.layers.4.fc1.weight\n",
            "Count :  77  name :  model.encoder.layers.4.fc1.bias\n",
            "Count :  78  name :  model.encoder.layers.4.fc2.weight\n",
            "Count :  79  name :  model.encoder.layers.4.fc2.bias\n",
            "Count :  80  name :  model.encoder.layers.4.final_layer_norm.weight\n",
            "Count :  81  name :  model.encoder.layers.4.final_layer_norm.bias\n",
            "Count :  82  name :  model.encoder.layers.5.self_attn.k_proj.weight\n",
            "Count :  83  name :  model.encoder.layers.5.self_attn.k_proj.bias\n",
            "Count :  84  name :  model.encoder.layers.5.self_attn.v_proj.weight\n",
            "Count :  85  name :  model.encoder.layers.5.self_attn.v_proj.bias\n",
            "Count :  86  name :  model.encoder.layers.5.self_attn.q_proj.weight\n",
            "Count :  87  name :  model.encoder.layers.5.self_attn.q_proj.bias\n",
            "Count :  88  name :  model.encoder.layers.5.self_attn.out_proj.weight\n",
            "Count :  89  name :  model.encoder.layers.5.self_attn.out_proj.bias\n",
            "Count :  90  name :  model.encoder.layers.5.self_attn_layer_norm.weight\n",
            "Count :  91  name :  model.encoder.layers.5.self_attn_layer_norm.bias\n",
            "Count :  92  name :  model.encoder.layers.5.fc1.weight\n",
            "Count :  93  name :  model.encoder.layers.5.fc1.bias\n",
            "Count :  94  name :  model.encoder.layers.5.fc2.weight\n",
            "Count :  95  name :  model.encoder.layers.5.fc2.bias\n",
            "Count :  96  name :  model.encoder.layers.5.final_layer_norm.weight\n",
            "Count :  97  name :  model.encoder.layers.5.final_layer_norm.bias\n",
            "Count :  98  name :  model.encoder.layernorm_embedding.weight\n",
            "Count :  99  name :  model.encoder.layernorm_embedding.bias\n",
            "Count :  100  name :  model.encoder.MAF_layer4.acoustic_context_transform.weight\n",
            "Count :  101  name :  model.encoder.MAF_layer4.acoustic_context_attention.attention_layer.in_proj_weight\n",
            "Count :  102  name :  model.encoder.MAF_layer4.acoustic_context_attention.attention_layer.in_proj_bias\n",
            "Count :  103  name :  model.encoder.MAF_layer4.acoustic_context_attention.attention_layer.out_proj.weight\n",
            "Count :  104  name :  model.encoder.MAF_layer4.acoustic_context_attention.attention_layer.out_proj.bias\n",
            "Count :  105  name :  model.encoder.MAF_layer4.acoustic_context_attention.u_k.weight\n",
            "Count :  106  name :  model.encoder.MAF_layer4.acoustic_context_attention.w1_k.weight\n",
            "Count :  107  name :  model.encoder.MAF_layer4.acoustic_context_attention.w2_k.weight\n",
            "Count :  108  name :  model.encoder.MAF_layer4.acoustic_context_attention.u_v.weight\n",
            "Count :  109  name :  model.encoder.MAF_layer4.acoustic_context_attention.w1_v.weight\n",
            "Count :  110  name :  model.encoder.MAF_layer4.acoustic_context_attention.w2_v.weight\n",
            "Count :  111  name :  model.encoder.MAF_layer4.acoustic_gate.weight\n",
            "Count :  112  name :  model.encoder.MAF_layer4.acoustic_gate.bias\n",
            "Count :  113  name :  model.encoder.MAF_layer4.final_layer_norm.weight\n",
            "Count :  114  name :  model.encoder.MAF_layer4.final_layer_norm.bias\n",
            "Count :  115  name :  model.encoder.MAF_layer5.visual_context_transform.weight\n",
            "Count :  116  name :  model.encoder.MAF_layer5.visual_context_attention.attention_layer.in_proj_weight\n",
            "Count :  117  name :  model.encoder.MAF_layer5.visual_context_attention.attention_layer.in_proj_bias\n",
            "Count :  118  name :  model.encoder.MAF_layer5.visual_context_attention.attention_layer.out_proj.weight\n",
            "Count :  119  name :  model.encoder.MAF_layer5.visual_context_attention.attention_layer.out_proj.bias\n",
            "Count :  120  name :  model.encoder.MAF_layer5.visual_context_attention.u_k.weight\n",
            "Count :  121  name :  model.encoder.MAF_layer5.visual_context_attention.w1_k.weight\n",
            "Count :  122  name :  model.encoder.MAF_layer5.visual_context_attention.w2_k.weight\n",
            "Count :  123  name :  model.encoder.MAF_layer5.visual_context_attention.u_v.weight\n",
            "Count :  124  name :  model.encoder.MAF_layer5.visual_context_attention.w1_v.weight\n",
            "Count :  125  name :  model.encoder.MAF_layer5.visual_context_attention.w2_v.weight\n",
            "Count :  126  name :  model.encoder.MAF_layer5.visual_gate.weight\n",
            "Count :  127  name :  model.encoder.MAF_layer5.visual_gate.bias\n",
            "Count :  128  name :  model.encoder.MAF_layer5.final_layer_norm.weight\n",
            "Count :  129  name :  model.encoder.MAF_layer5.final_layer_norm.bias\n",
            "Count :  130  name :  model.decoder.embed_positions.weight\n",
            "Count :  131  name :  model.decoder.layers.0.self_attn.k_proj.weight\n",
            "Count :  132  name :  model.decoder.layers.0.self_attn.k_proj.bias\n",
            "Count :  133  name :  model.decoder.layers.0.self_attn.v_proj.weight\n",
            "Count :  134  name :  model.decoder.layers.0.self_attn.v_proj.bias\n",
            "Count :  135  name :  model.decoder.layers.0.self_attn.q_proj.weight\n",
            "Count :  136  name :  model.decoder.layers.0.self_attn.q_proj.bias\n",
            "Count :  137  name :  model.decoder.layers.0.self_attn.out_proj.weight\n",
            "Count :  138  name :  model.decoder.layers.0.self_attn.out_proj.bias\n",
            "Count :  139  name :  model.decoder.layers.0.self_attn_layer_norm.weight\n",
            "Count :  140  name :  model.decoder.layers.0.self_attn_layer_norm.bias\n",
            "Count :  141  name :  model.decoder.layers.0.encoder_attn.k_proj.weight\n",
            "Count :  142  name :  model.decoder.layers.0.encoder_attn.k_proj.bias\n",
            "Count :  143  name :  model.decoder.layers.0.encoder_attn.v_proj.weight\n",
            "Count :  144  name :  model.decoder.layers.0.encoder_attn.v_proj.bias\n",
            "Count :  145  name :  model.decoder.layers.0.encoder_attn.q_proj.weight\n",
            "Count :  146  name :  model.decoder.layers.0.encoder_attn.q_proj.bias\n",
            "Count :  147  name :  model.decoder.layers.0.encoder_attn.out_proj.weight\n",
            "Count :  148  name :  model.decoder.layers.0.encoder_attn.out_proj.bias\n",
            "Count :  149  name :  model.decoder.layers.0.encoder_attn_layer_norm.weight\n",
            "Count :  150  name :  model.decoder.layers.0.encoder_attn_layer_norm.bias\n",
            "Count :  151  name :  model.decoder.layers.0.fc1.weight\n",
            "Count :  152  name :  model.decoder.layers.0.fc1.bias\n",
            "Count :  153  name :  model.decoder.layers.0.fc2.weight\n",
            "Count :  154  name :  model.decoder.layers.0.fc2.bias\n",
            "Count :  155  name :  model.decoder.layers.0.final_layer_norm.weight\n",
            "Count :  156  name :  model.decoder.layers.0.final_layer_norm.bias\n",
            "Count :  157  name :  model.decoder.layers.1.self_attn.k_proj.weight\n",
            "Count :  158  name :  model.decoder.layers.1.self_attn.k_proj.bias\n",
            "Count :  159  name :  model.decoder.layers.1.self_attn.v_proj.weight\n",
            "Count :  160  name :  model.decoder.layers.1.self_attn.v_proj.bias\n",
            "Count :  161  name :  model.decoder.layers.1.self_attn.q_proj.weight\n",
            "Count :  162  name :  model.decoder.layers.1.self_attn.q_proj.bias\n",
            "Count :  163  name :  model.decoder.layers.1.self_attn.out_proj.weight\n",
            "Count :  164  name :  model.decoder.layers.1.self_attn.out_proj.bias\n",
            "Count :  165  name :  model.decoder.layers.1.self_attn_layer_norm.weight\n",
            "Count :  166  name :  model.decoder.layers.1.self_attn_layer_norm.bias\n",
            "Count :  167  name :  model.decoder.layers.1.encoder_attn.k_proj.weight\n",
            "Count :  168  name :  model.decoder.layers.1.encoder_attn.k_proj.bias\n",
            "Count :  169  name :  model.decoder.layers.1.encoder_attn.v_proj.weight\n",
            "Count :  170  name :  model.decoder.layers.1.encoder_attn.v_proj.bias\n",
            "Count :  171  name :  model.decoder.layers.1.encoder_attn.q_proj.weight\n",
            "Count :  172  name :  model.decoder.layers.1.encoder_attn.q_proj.bias\n",
            "Count :  173  name :  model.decoder.layers.1.encoder_attn.out_proj.weight\n",
            "Count :  174  name :  model.decoder.layers.1.encoder_attn.out_proj.bias\n",
            "Count :  175  name :  model.decoder.layers.1.encoder_attn_layer_norm.weight\n",
            "Count :  176  name :  model.decoder.layers.1.encoder_attn_layer_norm.bias\n",
            "Count :  177  name :  model.decoder.layers.1.fc1.weight\n",
            "Count :  178  name :  model.decoder.layers.1.fc1.bias\n",
            "Count :  179  name :  model.decoder.layers.1.fc2.weight\n",
            "Count :  180  name :  model.decoder.layers.1.fc2.bias\n",
            "Count :  181  name :  model.decoder.layers.1.final_layer_norm.weight\n",
            "Count :  182  name :  model.decoder.layers.1.final_layer_norm.bias\n",
            "Count :  183  name :  model.decoder.layers.2.self_attn.k_proj.weight\n",
            "Count :  184  name :  model.decoder.layers.2.self_attn.k_proj.bias\n",
            "Count :  185  name :  model.decoder.layers.2.self_attn.v_proj.weight\n",
            "Count :  186  name :  model.decoder.layers.2.self_attn.v_proj.bias\n",
            "Count :  187  name :  model.decoder.layers.2.self_attn.q_proj.weight\n",
            "Count :  188  name :  model.decoder.layers.2.self_attn.q_proj.bias\n",
            "Count :  189  name :  model.decoder.layers.2.self_attn.out_proj.weight\n",
            "Count :  190  name :  model.decoder.layers.2.self_attn.out_proj.bias\n",
            "Count :  191  name :  model.decoder.layers.2.self_attn_layer_norm.weight\n",
            "Count :  192  name :  model.decoder.layers.2.self_attn_layer_norm.bias\n",
            "Count :  193  name :  model.decoder.layers.2.encoder_attn.k_proj.weight\n",
            "Count :  194  name :  model.decoder.layers.2.encoder_attn.k_proj.bias\n",
            "Count :  195  name :  model.decoder.layers.2.encoder_attn.v_proj.weight\n",
            "Count :  196  name :  model.decoder.layers.2.encoder_attn.v_proj.bias\n",
            "Count :  197  name :  model.decoder.layers.2.encoder_attn.q_proj.weight\n",
            "Count :  198  name :  model.decoder.layers.2.encoder_attn.q_proj.bias\n",
            "Count :  199  name :  model.decoder.layers.2.encoder_attn.out_proj.weight\n",
            "Count :  200  name :  model.decoder.layers.2.encoder_attn.out_proj.bias\n",
            "Count :  201  name :  model.decoder.layers.2.encoder_attn_layer_norm.weight\n",
            "Count :  202  name :  model.decoder.layers.2.encoder_attn_layer_norm.bias\n",
            "Count :  203  name :  model.decoder.layers.2.fc1.weight\n",
            "Count :  204  name :  model.decoder.layers.2.fc1.bias\n",
            "Count :  205  name :  model.decoder.layers.2.fc2.weight\n",
            "Count :  206  name :  model.decoder.layers.2.fc2.bias\n",
            "Count :  207  name :  model.decoder.layers.2.final_layer_norm.weight\n",
            "Count :  208  name :  model.decoder.layers.2.final_layer_norm.bias\n",
            "Count :  209  name :  model.decoder.layers.3.self_attn.k_proj.weight\n",
            "Count :  210  name :  model.decoder.layers.3.self_attn.k_proj.bias\n",
            "Count :  211  name :  model.decoder.layers.3.self_attn.v_proj.weight\n",
            "Count :  212  name :  model.decoder.layers.3.self_attn.v_proj.bias\n",
            "Count :  213  name :  model.decoder.layers.3.self_attn.q_proj.weight\n",
            "Count :  214  name :  model.decoder.layers.3.self_attn.q_proj.bias\n",
            "Count :  215  name :  model.decoder.layers.3.self_attn.out_proj.weight\n",
            "Count :  216  name :  model.decoder.layers.3.self_attn.out_proj.bias\n",
            "Count :  217  name :  model.decoder.layers.3.self_attn_layer_norm.weight\n",
            "Count :  218  name :  model.decoder.layers.3.self_attn_layer_norm.bias\n",
            "Count :  219  name :  model.decoder.layers.3.encoder_attn.k_proj.weight\n",
            "Count :  220  name :  model.decoder.layers.3.encoder_attn.k_proj.bias\n",
            "Count :  221  name :  model.decoder.layers.3.encoder_attn.v_proj.weight\n",
            "Count :  222  name :  model.decoder.layers.3.encoder_attn.v_proj.bias\n",
            "Count :  223  name :  model.decoder.layers.3.encoder_attn.q_proj.weight\n",
            "Count :  224  name :  model.decoder.layers.3.encoder_attn.q_proj.bias\n",
            "Count :  225  name :  model.decoder.layers.3.encoder_attn.out_proj.weight\n",
            "Count :  226  name :  model.decoder.layers.3.encoder_attn.out_proj.bias\n",
            "Count :  227  name :  model.decoder.layers.3.encoder_attn_layer_norm.weight\n",
            "Count :  228  name :  model.decoder.layers.3.encoder_attn_layer_norm.bias\n",
            "Count :  229  name :  model.decoder.layers.3.fc1.weight\n",
            "Count :  230  name :  model.decoder.layers.3.fc1.bias\n",
            "Count :  231  name :  model.decoder.layers.3.fc2.weight\n",
            "Count :  232  name :  model.decoder.layers.3.fc2.bias\n",
            "Count :  233  name :  model.decoder.layers.3.final_layer_norm.weight\n",
            "Count :  234  name :  model.decoder.layers.3.final_layer_norm.bias\n",
            "Count :  235  name :  model.decoder.layers.4.self_attn.k_proj.weight\n",
            "Count :  236  name :  model.decoder.layers.4.self_attn.k_proj.bias\n",
            "Count :  237  name :  model.decoder.layers.4.self_attn.v_proj.weight\n",
            "Count :  238  name :  model.decoder.layers.4.self_attn.v_proj.bias\n",
            "Count :  239  name :  model.decoder.layers.4.self_attn.q_proj.weight\n",
            "Count :  240  name :  model.decoder.layers.4.self_attn.q_proj.bias\n",
            "Count :  241  name :  model.decoder.layers.4.self_attn.out_proj.weight\n",
            "Count :  242  name :  model.decoder.layers.4.self_attn.out_proj.bias\n",
            "Count :  243  name :  model.decoder.layers.4.self_attn_layer_norm.weight\n",
            "Count :  244  name :  model.decoder.layers.4.self_attn_layer_norm.bias\n",
            "Count :  245  name :  model.decoder.layers.4.encoder_attn.k_proj.weight\n",
            "Count :  246  name :  model.decoder.layers.4.encoder_attn.k_proj.bias\n",
            "Count :  247  name :  model.decoder.layers.4.encoder_attn.v_proj.weight\n",
            "Count :  248  name :  model.decoder.layers.4.encoder_attn.v_proj.bias\n",
            "Count :  249  name :  model.decoder.layers.4.encoder_attn.q_proj.weight\n",
            "Count :  250  name :  model.decoder.layers.4.encoder_attn.q_proj.bias\n",
            "Count :  251  name :  model.decoder.layers.4.encoder_attn.out_proj.weight\n",
            "Count :  252  name :  model.decoder.layers.4.encoder_attn.out_proj.bias\n",
            "Count :  253  name :  model.decoder.layers.4.encoder_attn_layer_norm.weight\n",
            "Count :  254  name :  model.decoder.layers.4.encoder_attn_layer_norm.bias\n",
            "Count :  255  name :  model.decoder.layers.4.fc1.weight\n",
            "Count :  256  name :  model.decoder.layers.4.fc1.bias\n",
            "Count :  257  name :  model.decoder.layers.4.fc2.weight\n",
            "Count :  258  name :  model.decoder.layers.4.fc2.bias\n",
            "Count :  259  name :  model.decoder.layers.4.final_layer_norm.weight\n",
            "Count :  260  name :  model.decoder.layers.4.final_layer_norm.bias\n",
            "Count :  261  name :  model.decoder.layers.5.self_attn.k_proj.weight\n",
            "Count :  262  name :  model.decoder.layers.5.self_attn.k_proj.bias\n",
            "Count :  263  name :  model.decoder.layers.5.self_attn.v_proj.weight\n",
            "Count :  264  name :  model.decoder.layers.5.self_attn.v_proj.bias\n",
            "Count :  265  name :  model.decoder.layers.5.self_attn.q_proj.weight\n",
            "Count :  266  name :  model.decoder.layers.5.self_attn.q_proj.bias\n",
            "Count :  267  name :  model.decoder.layers.5.self_attn.out_proj.weight\n",
            "Count :  268  name :  model.decoder.layers.5.self_attn.out_proj.bias\n",
            "Count :  269  name :  model.decoder.layers.5.self_attn_layer_norm.weight\n",
            "Count :  270  name :  model.decoder.layers.5.self_attn_layer_norm.bias\n",
            "Count :  271  name :  model.decoder.layers.5.encoder_attn.k_proj.weight\n",
            "Count :  272  name :  model.decoder.layers.5.encoder_attn.k_proj.bias\n",
            "Count :  273  name :  model.decoder.layers.5.encoder_attn.v_proj.weight\n",
            "Count :  274  name :  model.decoder.layers.5.encoder_attn.v_proj.bias\n",
            "Count :  275  name :  model.decoder.layers.5.encoder_attn.q_proj.weight\n",
            "Count :  276  name :  model.decoder.layers.5.encoder_attn.q_proj.bias\n",
            "Count :  277  name :  model.decoder.layers.5.encoder_attn.out_proj.weight\n",
            "Count :  278  name :  model.decoder.layers.5.encoder_attn.out_proj.bias\n",
            "Count :  279  name :  model.decoder.layers.5.encoder_attn_layer_norm.weight\n",
            "Count :  280  name :  model.decoder.layers.5.encoder_attn_layer_norm.bias\n",
            "Count :  281  name :  model.decoder.layers.5.fc1.weight\n",
            "Count :  282  name :  model.decoder.layers.5.fc1.bias\n",
            "Count :  283  name :  model.decoder.layers.5.fc2.weight\n",
            "Count :  284  name :  model.decoder.layers.5.fc2.bias\n",
            "Count :  285  name :  model.decoder.layers.5.final_layer_norm.weight\n",
            "Count :  286  name :  model.decoder.layers.5.final_layer_norm.bias\n",
            "Count :  287  name :  model.decoder.layernorm_embedding.weight\n",
            "Count :  288  name :  model.decoder.layernorm_embedding.bias\n",
            "Count :  289  name :  classification_head.dense.weight\n",
            "Count :  290  name :  classification_head.dense.bias\n",
            "Count :  291  name :  classification_head.out_proj.weight\n",
            "Count :  292  name :  classification_head.out_proj.bias\n"
          ]
        }
      ],
      "source": [
        "cnt = 0\n",
        "for name, param in model.named_parameters():\n",
        "    print(\"Count : \", cnt, \" name : \", name)\n",
        "    cnt+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "feA-7jSwIBQO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count :  100  name :  model.encoder.MAF_layer4.acoustic_context_transform.weight\n",
            "Count :  101  name :  model.encoder.MAF_layer4.acoustic_context_attention.attention_layer.in_proj_weight\n",
            "Count :  102  name :  model.encoder.MAF_layer4.acoustic_context_attention.attention_layer.in_proj_bias\n",
            "Count :  103  name :  model.encoder.MAF_layer4.acoustic_context_attention.attention_layer.out_proj.weight\n",
            "Count :  104  name :  model.encoder.MAF_layer4.acoustic_context_attention.attention_layer.out_proj.bias\n",
            "Count :  105  name :  model.encoder.MAF_layer4.acoustic_context_attention.u_k.weight\n",
            "Count :  106  name :  model.encoder.MAF_layer4.acoustic_context_attention.w1_k.weight\n",
            "Count :  107  name :  model.encoder.MAF_layer4.acoustic_context_attention.w2_k.weight\n",
            "Count :  108  name :  model.encoder.MAF_layer4.acoustic_context_attention.u_v.weight\n",
            "Count :  109  name :  model.encoder.MAF_layer4.acoustic_context_attention.w1_v.weight\n",
            "Count :  110  name :  model.encoder.MAF_layer4.acoustic_context_attention.w2_v.weight\n",
            "Count :  111  name :  model.encoder.MAF_layer4.acoustic_gate.weight\n",
            "Count :  112  name :  model.encoder.MAF_layer4.acoustic_gate.bias\n",
            "Count :  113  name :  model.encoder.MAF_layer4.final_layer_norm.weight\n",
            "Count :  114  name :  model.encoder.MAF_layer4.final_layer_norm.bias\n",
            "Count :  115  name :  model.encoder.MAF_layer5.visual_context_transform.weight\n",
            "Count :  116  name :  model.encoder.MAF_layer5.visual_context_attention.attention_layer.in_proj_weight\n",
            "Count :  117  name :  model.encoder.MAF_layer5.visual_context_attention.attention_layer.in_proj_bias\n",
            "Count :  118  name :  model.encoder.MAF_layer5.visual_context_attention.attention_layer.out_proj.weight\n",
            "Count :  119  name :  model.encoder.MAF_layer5.visual_context_attention.attention_layer.out_proj.bias\n",
            "Count :  120  name :  model.encoder.MAF_layer5.visual_context_attention.u_k.weight\n",
            "Count :  121  name :  model.encoder.MAF_layer5.visual_context_attention.w1_k.weight\n",
            "Count :  122  name :  model.encoder.MAF_layer5.visual_context_attention.w2_k.weight\n",
            "Count :  123  name :  model.encoder.MAF_layer5.visual_context_attention.u_v.weight\n",
            "Count :  124  name :  model.encoder.MAF_layer5.visual_context_attention.w1_v.weight\n",
            "Count :  125  name :  model.encoder.MAF_layer5.visual_context_attention.w2_v.weight\n",
            "Count :  126  name :  model.encoder.MAF_layer5.visual_gate.weight\n",
            "Count :  127  name :  model.encoder.MAF_layer5.visual_gate.bias\n",
            "Count :  128  name :  model.encoder.MAF_layer5.final_layer_norm.weight\n",
            "Count :  129  name :  model.encoder.MAF_layer5.final_layer_norm.bias\n",
            "Count :  289  name :  classification_head.dense.weight\n",
            "Count :  290  name :  classification_head.dense.bias\n",
            "Count :  291  name :  classification_head.out_proj.weight\n",
            "Count :  292  name :  classification_head.out_proj.bias\n",
            "Total trainanable parameters :  12.75229\n"
          ]
        }
      ],
      "source": [
        "cnt = 0\n",
        "for name, param in model.named_parameters():\n",
        "    # print(\"Count : \", cnt, \" name : \", name)\n",
        "    # if(cnt>=100 and cnt<=177):\n",
        "    # if(cnt>=66 and cnt<=151):\n",
        "    # if(cnt>=100 and cnt<=179):\n",
        "    # if(cnt>=100 and cnt<=155):\n",
        "    # if(cnt>=100 and cnt<=189):\n",
        "    if(cnt>=100 and cnt<=129):\n",
        "    # if(cnt>=100 and cnt<=125):\n",
        "        param.requires_grad = True\n",
        "        print(\"Count : \", cnt, \" name : \", name)\n",
        "    # elif(cnt>=337):\n",
        "    # elif(cnt>=343):\n",
        "    # elif(cnt>=315):\n",
        "    # elif(cnt>=329 and cnt<=332):\n",
        "    #     param.requires_grad = True\n",
        "    #     print(\"Count : \", cnt, \" name : \", name)\n",
        "    elif(cnt>=289):\n",
        "    # elif(cnt>=311):\n",
        "    # elif(cnt>=285):\n",
        "        param.requires_grad = True\n",
        "        print(\"Count : \", cnt, \" name : \", name)\n",
        "\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "    cnt+=1\n",
        "\n",
        "\n",
        "num_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total trainanable parameters : \", num_param/1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46OM_4HZjKNp"
      },
      "outputs": [],
      "source": [
        "# with open(\"/content/drive/MyDrive/Colab Notebooks/32/json_file_fold.p\", \"rb\") as f:\n",
        "#     text_file = pickle.load(f)\n",
        "\n",
        "# train_text = text_file[\"json_file_list_\" + str(foldNum) +\"_train\"]\n",
        "# test_text =  text_file[\"json_file_list_\" + str(foldNum) +\"_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "984ZSUXVWQfj"
      },
      "outputs": [],
      "source": [
        "# train_audio_data_utterance = torch.tensor(train_audio_data_utterance1)\n",
        "# # train_audio_data_utterance = train_audio_data_utterance.unsqueeze(dim = 1)\n",
        "# train_audio_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AleXnjQNWTKL"
      },
      "outputs": [],
      "source": [
        "# train_image_data_utterance = torch.tensor(train_image_data_utterance1)\n",
        "# # train_image_data_utterance = train_image_data_utterance.unsqueeze(dim = 1)\n",
        "# train_image_data_utterance.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "X0y5DYklxuUT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BartTokenizerFast(name_or_path='facebook/bart-base', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['[CONTEXT]', '[UTTERANCE]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
              "\t50265: AddedToken(\"[CONTEXT]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t50266: AddedToken(\"[UTTERANCE]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p = {\n",
        "        'additional_special_tokens' : ['[CONTEXT]', '[UTTERANCE]']\n",
        "    }\n",
        "\n",
        "tokenizer.add_special_tokens(p)\n",
        "\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "P1Ctmwokx-T2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(50267, 768)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNX4EgOarpN_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiN-BMSgISy7"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(text_data):\n",
        "    dialog = []\n",
        "    labels = []\n",
        "    sample_utterances = []\n",
        "    for i in range(len(text_data)):\n",
        "        data_point = text_data[i]\n",
        "\n",
        "        example_speaker = data_point['speaker']\n",
        "        example_utterance = data_point['utterance']\n",
        "        temp_label = int(data_point['sarcasm'])\n",
        "        sample_utterances.append(data_point)\n",
        "\n",
        "        # example_dialog = '[CONTEXT] '\n",
        "        # example_dialog = '[TARGET] '\n",
        "        example_dialog = '[CONTEXT] '\n",
        "\n",
        "\n",
        "        for speaker, utterance in list(zip(data_point['context_speakers'], data_point['context'])):\n",
        "            example_dialog = example_dialog + speaker.upper() + \" : \" + utterance + \" | \"\n",
        "\n",
        "        example_dialog = example_dialog + ' [UTTERANCE] ' + example_speaker + \" : \" + example_utterance + \" | \"\n",
        "        # example_dialog = example_dialog + example_speaker + \" : \" + example_utterance\n",
        "        # example_dialog = example_dialog + example_speaker + \" : \" + example_utterance\n",
        "        # print(example_dialog)\n",
        "        example_dialog = re.sub(' +', ' ', example_dialog)\n",
        "\n",
        "        dialog.append(example_dialog)\n",
        "        labels.append(temp_label)\n",
        "\n",
        "        print(data_point)\n",
        "        print(temp_label)\n",
        "\n",
        "    # df = pd.DataFrame(dialog, columns=['dialog'])\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "    enc = tokenizer(dialog, max_length = SOURCE_MAX_LEN, padding = 'max_length', truncation = True)\n",
        "\n",
        "    # df['audio_features'] = acoustic_data\n",
        "    # df['visual_features'] = visual_data\n",
        "\n",
        "    return torch.tensor(enc['input_ids'], dtype=torch.long), torch.tensor(enc['attention_mask'], dtype=torch.bool), sample_utterances, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2dU7MfiZaDq"
      },
      "outputs": [],
      "source": [
        "class HateMMDataset(data.Dataset):\n",
        "    \"Characterizes a dataset for PyTorch\"\n",
        "    def __init__(self, folders, labels):\n",
        "        \"Initialization\"\n",
        "        # print(folders, labels)\n",
        "        self.labels = labels\n",
        "        self.folders = folders\n",
        "\n",
        "    def __len__(self):\n",
        "        \"Denotes the total number of samples\"\n",
        "        return len(self.folders)\n",
        "\n",
        "    def load_data_for_video(self, video):\n",
        "        video_file_name_without_extension, _ = os.path.splitext(video)\n",
        "        pickle_file_path = os.path.join(FOLDER_NAME, \"VITF_new\", video_file_name_without_extension + \"_vit.p\")\n",
        "        \n",
        "        # Load text data\n",
        "        if video in transcript:\n",
        "            text_features = torch.tensor(np.array(transcript[video]), dtype=torch.float32)\n",
        "        else:\n",
        "            raise ValueError(f\"Text data not found for {video}\")\n",
        "        \n",
        "        # Load video data\n",
        "        try:\n",
        "            with open(pickle_file_path, 'rb') as fp:\n",
        "                video_data = pickle.load(fp)\n",
        "                video_features = torch.tensor(np.array(list(video_data.values())), dtype=torch.float32)\n",
        "        except FileNotFoundError:\n",
        "            raise ValueError(f\"Video data file not found: {pickle_file_path}\")\n",
        "        \n",
        "        # Load audio data\n",
        "        if video in audio_data:\n",
        "            audio_features = torch.tensor(np.array(audio_data[video]), dtype=torch.float32)\n",
        "            audio_features = audio_features.mean(dim=0).unsqueeze(0)\n",
        "            # audio_features, _ = audio_features.max(dim=0)\n",
        "        else:\n",
        "            raise ValueError(f\"Audio data not found for {video}\")\n",
        "        \n",
        "        return text_features, video_features, audio_features\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"Generates one sample of data\"\n",
        "        try:\n",
        "            # Select sample\n",
        "            folder = self.folders[index]\n",
        "            # Load data\n",
        "            X_text, X_vid, X_audio = self.load_data_for_video(folder)\n",
        "            y = torch.LongTensor([self.labels[index]]) \n",
        "            return X_text, X_vid, X_audio, y\n",
        "        except Exception as e:\n",
        "            # traceback.print_exc()\n",
        "            print(f\"Error loading data for index {index}: {e}\")                        \n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7p229zVGBUG"
      },
      "outputs": [],
      "source": [
        "train_text_input_ids1, train_text_attention_mask1, train_utterances, train_ground_truth1 = prepare_dataset(train_text)\n",
        "train_ground_truth1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vB39rShGL4K"
      },
      "outputs": [],
      "source": [
        "test_text_input_ids, test_text_attention_mask, test_utterances1, test_ground_truth = prepare_dataset(test_text)\n",
        "test_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7uPY9VkLRcr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGdaP4uxzX05"
      },
      "outputs": [],
      "source": [
        "test_input_data = []\n",
        "\n",
        "\n",
        "for j in range(test_ground_truth.shape[0]):\n",
        "  temp_list = []\n",
        "  temp_list.append(test_text_input_ids[j])\n",
        "  temp_list.append(test_text_attention_mask[j])\n",
        "\n",
        "  temp_list.append(test_audio_data_utterance1[j])\n",
        "  temp_list.append(test_image_data_utterance1[j])\n",
        "  temp_list.append(test_utterances1[j])\n",
        "  temp_list.append(test_dlog_id[j])\n",
        "\n",
        "  test_input_data.append(temp_list)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Vp5_UG4zm_e"
      },
      "outputs": [],
      "source": [
        "test_output_data = test_ground_truth.tolist()\n",
        "print(type(test_output_data))\n",
        "print(len(test_output_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OYmBDf1zpVv"
      },
      "outputs": [],
      "source": [
        "X_valid, X_test, Y_valid, Y_test = train_test_split(\n",
        "    test_input_data, test_output_data, test_size = 0.5, stratify = test_output_data\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_TuTdsjz2gx"
      },
      "outputs": [],
      "source": [
        "valid_text_input_ids = []\n",
        "valid_text_attention_mask = []\n",
        "valid_context_input_ids = []\n",
        "valid_context_attention_mask = []\n",
        "\n",
        "valid_audio_data = []\n",
        "valid_image_data = []\n",
        "\n",
        "\n",
        "\n",
        "valid_utterances_data = []\n",
        "valid_did = []\n",
        "\n",
        "for j in range(len(X_valid)):\n",
        "  valid_text_input_ids.append(X_valid[j][0])\n",
        "  valid_text_attention_mask.append(X_valid[j][1])\n",
        "\n",
        "\n",
        "  valid_audio_data.append(X_valid[j][2])\n",
        "\n",
        "  valid_image_data.append(X_valid[j][3])\n",
        "  valid_utterances_data.append(X_valid[j][4])\n",
        "  valid_did.append(X_valid[j][5])\n",
        "\n",
        "print(len(valid_text_input_ids))\n",
        "print(len(valid_text_attention_mask))\n",
        "\n",
        "print(len(valid_audio_data))\n",
        "print(len(valid_image_data))\n",
        "\n",
        "print(len(valid_utterances_data))\n",
        "print(len(valid_did))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_RHsBNLz_d8"
      },
      "outputs": [],
      "source": [
        "valid_text_input_ids = torch.stack(valid_text_input_ids)\n",
        "print(valid_text_input_ids.shape)\n",
        "valid_text_attention_mask = torch.stack(valid_text_attention_mask)\n",
        "print(valid_text_attention_mask.shape)\n",
        "\n",
        "valid_audio_data = torch.stack(valid_audio_data)\n",
        "print(valid_audio_data.shape)\n",
        "valid_image_data = torch.stack(valid_image_data)\n",
        "print(valid_image_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctw1YWnX0DrP"
      },
      "outputs": [],
      "source": [
        "valid_ground_truth = torch.tensor(Y_valid)\n",
        "valid_ground_truth.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ogsa3Gqd0FvA"
      },
      "outputs": [],
      "source": [
        "test_text_id = []\n",
        "test_text_mask = []\n",
        "\n",
        "test_context_id = []\n",
        "test_context_mask = []\n",
        "\n",
        "test_audio_data = []\n",
        "test_image_data = []\n",
        "\n",
        "test_utterances_data = []\n",
        "test_did = []\n",
        "for j in range(len(X_test)):\n",
        "  test_text_id.append(X_test[j][0])\n",
        "  test_text_mask.append(X_test[j][1])\n",
        "\n",
        "\n",
        "  test_audio_data.append(X_test[j][2])\n",
        "\n",
        "  test_image_data.append(X_test[j][3])\n",
        "  test_utterances_data.append(X_test[j][4])\n",
        "  test_did.append(X_test[j][5])\n",
        "\n",
        "test_text_id = torch.stack(test_text_id)\n",
        "print(test_text_id.shape)\n",
        "test_text_mask = torch.stack(test_text_mask)\n",
        "print(test_text_mask.shape)\n",
        "\n",
        "test_audio_data = torch.stack(test_audio_data)\n",
        "print(test_audio_data.shape)\n",
        "test_image_data = torch.stack(test_image_data)\n",
        "print(test_image_data.shape)\n",
        "\n",
        "print(len(test_utterances_data))\n",
        "print(len(test_did))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uzCN2Pz0NdD"
      },
      "outputs": [],
      "source": [
        "test_ground_truth = torch.tensor(Y_test)\n",
        "print(test_ground_truth.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sU4-_R3mDbB"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/Colab Notebooks/32/test_did.p', 'wb') as f:\n",
        "#   pickle.dump(test_did, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQRYMjVOvJi1"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/Colab Notebooks/32/valid_did.p', 'wb') as f:\n",
        "#   pickle.dump(valid_did, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrligkveIb4f"
      },
      "outputs": [],
      "source": [
        "class MultimodalSarcasmDataset(Dataset):\n",
        "    # def __init__(self, utterance_input_ids, utterance_attention_mask, context_input_ids, context_attention_mask, acoustic_data, visual_data, labels):\n",
        "    def __init__(self, utterance_input_ids, utterance_attention_mask, acoustic_data, visual_data, labels):\n",
        "\n",
        "        self.utterance_input_ids = utterance_input_ids\n",
        "        self.utterance_attention_mask = utterance_attention_mask\n",
        "        # self.context_input_ids = context_input_ids\n",
        "        # self.context_attention_mask = context_attention_mask\n",
        "        # self.context_attention_mask\n",
        "        self.acoustic_data = acoustic_data\n",
        "        self.visual_data = visual_data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.utterance_input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # return self.utterance_input_ids[idx], self.utterance_attention_mask[idx], self.context_input_ids[idx], self.context_attention_mask[idx], self.acoustic_data[idx], self.visual_data[idx], self.labels[idx]\n",
        "        return self.utterance_input_ids[idx], self.utterance_attention_mask[idx],  self.acoustic_data[idx], self.visual_data[idx], self.labels[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1wRtyIFYS2m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j_r6ZqtIeD9"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_loader = DataLoader(MultimodalSarcasmDataset(train_text_input_ids, train_text_attention_mask, train_audio_data_utterance, train_image_data_utterance, train_ground_truth), batch_size=32, shuffle = True)\n",
        "valid_loader = DataLoader(MultimodalSarcasmDataset(valid_text_input_ids, valid_text_attention_mask, valid_audio_data, valid_image_data, valid_ground_truth), batch_size = 32, shuffle = False)\n",
        "test_loader = DataLoader(MultimodalSarcasmDataset(test_text_id, test_text_mask, test_audio_data, test_image_data, test_ground_truth), batch_size=32, shuffle = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44TjmFmSIh9D"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5MxPWWyIotE"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, data_loader):\n",
        "      model.train()\n",
        "      epoch_train_loss = 0.0\n",
        "\n",
        "\n",
        "      for step, batch in enumerate(tqdm(data_loader, desc = 'Training Iteration')):\n",
        "        # for i, t in enumerate(batch):\n",
        "        #     print(\"Inside hello\")\n",
        "        #     print(i, \" : \", type(t))\n",
        "        batch = tuple(t.to(DEVICE) for t in batch)\n",
        "        # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n",
        "        input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # print(\"Input ids shape : \", input_ids.shape)\n",
        "        # print(\"Input ids shape : \", input_ids.shape)\n",
        "        outputs = model(input_ids = input_ids,\n",
        "                        attention_mask = attention_mask,\n",
        "                        # context_input_ids = context_input_ids,\n",
        "                        # context_attention_mask = context_attention_mask,\n",
        "                        acoustic_input = acoustic_input,\n",
        "                        visual_input = visual_input,\n",
        "                        labels = labels)\n",
        "\n",
        "        loss = outputs['loss']\n",
        "        epoch_train_loss += loss.item()\n",
        "\n",
        "        # print(\"Batch wise loss : \", epoch_train_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "      print(\"Epoch train loss : \", epoch_train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0H6Hk2w62xu"
      },
      "outputs": [],
      "source": [
        "def valid_epoch(model, data_loader):\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  gold = []\n",
        "\n",
        "  valid_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(tqdm(data_loader)):\n",
        "      batch = tuple(t.to(DEVICE) for t in batch)\n",
        "      # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n",
        "      input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n",
        "\n",
        "      outputs = model(input_ids = input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            # context_input_ids = context_input_ids,\n",
        "                            # context_attention_mask = context_attention_mask,\n",
        "                            acoustic_input = acoustic_input,\n",
        "                            visual_input = visual_input,\n",
        "                            labels = labels)\n",
        "\n",
        "      logits = outputs['logits']\n",
        "      loss = outputs['loss']\n",
        "\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "      pred = logits.argmax(dim = -1)\n",
        "\n",
        "      predictions.extend(pred.tolist())\n",
        "      gold.extend(labels.tolist())\n",
        "\n",
        "  return valid_loss, predictions, gold\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93nGID9-Imr-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def test_epoch(model, data_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    gold = []\n",
        "\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(tqdm(data_loader)):\n",
        "            batch = tuple(t.to(DEVICE) for t in batch)\n",
        "            # input_ids, attention_mask, context_input_ids, context_attention_mask, acoustic_input, visual_input, labels = batch\n",
        "            input_ids, attention_mask,  acoustic_input, visual_input, labels = batch\n",
        "\n",
        "            outputs = model(input_ids = input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            # context_input_ids = context_input_ids,\n",
        "                            # context_attention_mask = context_attention_mask,\n",
        "\n",
        "                            acoustic_input = acoustic_input,\n",
        "                            visual_input = visual_input,\n",
        "                            labels = labels)\n",
        "\n",
        "            logits = outputs['logits']\n",
        "\n",
        "            pred = logits.argmax(dim = -1)\n",
        "\n",
        "            predictions.extend(pred.tolist())\n",
        "\n",
        "            gold.extend(labels.tolist())\n",
        "\n",
        "            correct += int((pred == labels).sum())\n",
        "\n",
        "    return correct/len(data_loader.dataset), predictions, gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5QauBgKBtYS"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "  def __init__(self, patience, min_delta):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.counter = 0\n",
        "    self.min_validation = np.inf\n",
        "\n",
        "  def early_stop(self, valid_loss):\n",
        "    if valid_loss < self.min_validation:\n",
        "      self.min_validation = valid_loss\n",
        "      self.counter = 0\n",
        "    elif valid_loss > (self.min_validation + self.min_delta):\n",
        "      self.counter += 1\n",
        "      if self.counter >= self.patience:\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dHmyfM2Cc9b"
      },
      "outputs": [],
      "source": [
        "early_stopper = EarlyStopping(patience = 15, min_delta = 0.2)\n",
        "early_stopper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0HB4PIc6ixU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_and_validation(model, train_loader, valid_loader):\n",
        "  # lowest_loss = 1e6\n",
        "  best_f1 = 0.0\n",
        "  # min_loss = 1e6\n",
        "  for epoch in range(30):\n",
        "    print(\"\\n=============Epoch : \", epoch)\n",
        "    train_epoch(model, train_loader)\n",
        "    valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "    if early_stopper.early_stop(valid_loss):\n",
        "      break\n",
        "\n",
        "    print(\"Length of predictions : \", len(valid_pred))\n",
        "    print(\"Length of gold : \", len(valid_gold))\n",
        "    print(\"Valid loss : \", valid_loss)\n",
        "    print(\"\\n Valid Accuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "    print(\"\\n Valid Precision : \", precision_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "    print(\"\\n Valid Recall : \", recall_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "    print(\"\\nValid F1 score : \", f1_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "\n",
        "\n",
        "    curr_f1 = f1_score(valid_gold, valid_pred, average = 'weighted')\n",
        "\n",
        "    curr_loss = valid_loss\n",
        "    # if((curr_f1 > best_f1) and (epoch>=4)):\n",
        "    if(curr_f1 > best_f1):\n",
        "    # if(curr_loss < min_loss):\n",
        "    # if(curr_loss < lowest_loss):\n",
        "      best_f1 = curr_f1\n",
        "      # min_loss = curr_loss\n",
        "      # print(\"Valid pred : \", valid_pred)\n",
        "      # print('valid_gold : ', valid_gold)\n",
        "      # torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/32/saved_model/best_case/best_model_epoch_'+str(epoch)+'_best_f1_'+str(int(best_f1*100))+'_foldNum_'+str(foldNum)+'.pt')\n",
        "      # print(\"model saved\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73CfE2olu5OS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMJL4HbkEBh2"
      },
      "outputs": [],
      "source": [
        "# train_and_validation(model, train_loader, test_loader)\n",
        "train_and_validation(model, train_loader, valid_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCxGBr5x6jcE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DO0Qsq9ufvhi"
      },
      "outputs": [],
      "source": [
        "# path = '/content/drive/MyDrive/Colab Notebooks/32/saved_model_f1/stratify/best_model_epoch_11_best_f1_78_foldNum_0.pt'\n",
        "\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/32/saved_model/best_case/best_model_epoch_8_best_f1_78_foldNum_0.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PMNVLL3BM-Y"
      },
      "outputs": [],
      "source": [
        "foldNum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J_qDFj5_ie3"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgEpzAt-_mSQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "acc, test_pred, test_gold = test_epoch(model, test_loader)\n",
        "\n",
        "print(acc)\n",
        "\n",
        "print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n",
        "print(\"\\nPrecision : \", precision_score(test_gold, test_pred, average = 'weighted'))\n",
        "print(\"\\nRecall : \", recall_score(test_gold, test_pred, average = 'weighted'))\n",
        "print(\"\\nF1 score : \", f1_score(test_gold, test_pred, average = 'weighted'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('\\nConfusion Matrix : ', confusion_matrix(test_gold, test_pred))\n",
        "\n",
        "sar = 0\n",
        "\n",
        "for j in range(len(test_pred)):\n",
        "  if((test_pred[j]==1) and (test_gold[j] == 1)):\n",
        "    sar+=1\n",
        "\n",
        "\n",
        "print(\"Sarcastic count : \", sar)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMc-bM2wbPdu"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/test_gold.p', 'wb') as f:\n",
        "  pickle.dump(test_gold, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gw-JGysfTIU"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/32/test_pred.p', 'wb') as f:\n",
        "  pickle.dump(test_pred, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50ctOavcBUOt"
      },
      "outputs": [],
      "source": [
        "for j in range(len(test_pred)):\n",
        "  if((test_pred[j]==0) and (test_gold[j] == 1)):\n",
        "    print(j,\" : \", test_did[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbj6bI_5uobi"
      },
      "outputs": [],
      "source": [
        "for j in range(len(test_pred)):\n",
        "  if((test_pred[j]==1) and (test_gold[j] == 0)):\n",
        "    print(j,\" : \", test_did[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2ZNDesvGfyT"
      },
      "outputs": [],
      "source": [
        "for j in range(len(test_pred)):\n",
        "  if((test_pred[j]==0) and (test_gold[j] == 0)):\n",
        "    print(j,\" : \", test_did[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A15BBLMrGhcg"
      },
      "outputs": [],
      "source": [
        "for j in range(len(test_pred)):\n",
        "  if((test_pred[j]==1) and (test_gold[j] == 1)):\n",
        "    print(j,\" : \", test_did[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eD8Mc-spvSz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuClass": "premium",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
