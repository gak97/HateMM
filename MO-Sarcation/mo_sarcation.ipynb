{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZQU-d0knGyhl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import warnings\n",
        "import logging\n",
        "import gc\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "from typing import Optional\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import sklearn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import jaccard_score, f1_score, accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r_Ovlo42G4Kh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "# from rouge_score.rouge_scorer import RougeScorer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from transformers import (\n",
        "    BartTokenizerFast,\n",
        "    AdamW\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    print(\"Using GPU\")\n",
        "\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "foldNum = 0\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "SOURCE_MAX_LEN = 768 # 500\n",
        "# TARGET_MAX_LEN = 50\n",
        "# MAX_UTTERANCES = 25\n",
        "\n",
        "ACOUSTIC_DIM = 768\n",
        "ACOUSTIC_MAX_LEN = 1000\n",
        "\n",
        "\n",
        "\n",
        "VISUAL_DIM = 768 # 2048\n",
        "VISUAL_MAX_LEN = 1000 # 480\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "\n",
        "VALID_LEN = 69\n",
        "\n",
        "# BASE_LEARNING_RATE = 5e-6\n",
        "# NEW_LEARNING_RATE = 5e-5\n",
        "# WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# NUM_BEAMS = 5\n",
        "# EARLY_STOPPING = True\n",
        "# NO_REPEAT_NGRAM_SIZE = 3\n",
        "\n",
        "# EARLY_STOPPING_THRESHOLD = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cMdRaTfAHF7Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed : 994\n"
          ]
        }
      ],
      "source": [
        "def set_random_seed(seed: int):\n",
        "    print(\"Seed : {}\".format(seed))\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# set_random_seed(42)\n",
        "# a = np.random.randint(0, 1000)\n",
        "\n",
        "# set_random_seed(123)\n",
        "set_random_seed(994)\n",
        "# set_random_seed(12345)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
        "\n",
        "from transformers.modeling_utils import PreTrainedModel, unwrap_model\n",
        "\n",
        "from transformers import (\n",
        "    BartTokenizer,\n",
        "    AdamW\n",
        ")\n",
        "import transformers\n",
        "from transformers.models.bart.configuration_bart import BartConfig\n",
        "\n",
        "from transformers.models.bart.modeling_bart import (\n",
        "    BartPretrainedModel,\n",
        "    BartDecoder,\n",
        "    BartLearnedPositionalEmbedding,\n",
        "    BartEncoderLayer,\n",
        "    shift_tokens_right,\n",
        ")\n",
        "\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutput,\n",
        "    Seq2SeqLMOutput,\n",
        "    Seq2SeqModelOutput,\n",
        "    Seq2SeqSequenceClassifierOutput\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e1XR3BlgHOY8"
      },
      "outputs": [],
      "source": [
        "BART_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
        "            it.\n",
        "            Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "            [What are input IDs?](../glossary#input-ids)\n",
        "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "            [What are attention masks?](../glossary#attention-mask)\n",
        "        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
        "            Indices of decoder input sequence tokens in the vocabulary.\n",
        "            Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "            [What are decoder input IDs?](../glossary#decoder-input-ids)\n",
        "            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n",
        "            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n",
        "            For translation and summarization training, `decoder_input_ids` should be provided. If no\n",
        "            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n",
        "            for denoising pre-training following the paper.\n",
        "        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
        "            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n",
        "            be used by default.\n",
        "            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n",
        "            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n",
        "            information on the default strategy.\n",
        "        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
        "            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n",
        "            1]`:\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n",
        "            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n",
        "            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n",
        "            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n",
        "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
        "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
        "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
        "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
        "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
        "            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
        "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
        "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
        "            `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of shape\n",
        "            `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing `input_ids` you\n",
        "            can choose to directly pass an embedded representation. This is useful if you want more control over how to\n",
        "            convert `input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n",
        "        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n",
        "            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n",
        "            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n",
        "            input (see `past_key_values`). This is useful if you want more control over how to convert\n",
        "            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n",
        "            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n",
        "            of `inputs_embeds`.\n",
        "        use_cache (`bool`, *optional*):\n",
        "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
        "            `past_key_values`).\n",
        "        output_attentions (`bool`, *optional*):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (`bool`, *optional*):\n",
        "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (`bool`, *optional*):\n",
        "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aEDR_7N6HWsu"
      },
      "outputs": [],
      "source": [
        "class ContextAwareAttention(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 dim_model : int,\n",
        "                 dim_context : int,\n",
        "                 dropout_rate : Optional[float] = 0.0 ):\n",
        "\n",
        "        super(ContextAwareAttention, self).__init__()\n",
        "\n",
        "        self.dim_model = dim_model\n",
        "        self.dim_context = dim_context\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.attention_layer = nn.MultiheadAttention(embed_dim=self.dim_model,\n",
        "                                                     num_heads = 1,\n",
        "                                                     dropout = self.dropout_rate,\n",
        "                                                     bias = True,\n",
        "                                                    add_zero_attn=False,\n",
        "                                                    batch_first=True,\n",
        "                                                    device=DEVICE\n",
        "        )\n",
        "\n",
        "        self.u_k = nn.Linear(self.dim_context, self.dim_model, bias = False)\n",
        "        self.w1_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "        self.w2_k = nn.Linear(self.dim_model, 1, bias=False)\n",
        "\n",
        "        self.u_v = nn.Linear(self.dim_context, self.dim_model, bias=False)\n",
        "        self.w1_v = nn.Linear(self.dim_model, 1, bias = False)\n",
        "        self.w2_v = nn.Linear(self.dim_model, 1, bias = False)\n",
        "\n",
        "    def forward(self, q, k, v, context):\n",
        "\n",
        "        # print(\"Context shape : \", context.shape)\n",
        "        # print(\"Dim context : \", self.dim_context, \" : Dim model : \", self.dim_model)\n",
        "        key_context = self.u_k(context)\n",
        "        # print(\"Context shape below key context : \", key_context.shape)\n",
        "        value_context = self.u_v(context)\n",
        "\n",
        "        lambda_k = F.sigmoid(self.w1_k(k) + self.w2_k(key_context))\n",
        "        lambda_v = F.sigmoid(self.w1_v(v) + self.w2_v(value_context))\n",
        "\n",
        "        k_cap = (1-lambda_k) * k + (lambda_k) * key_context\n",
        "        v_cap = (1-lambda_v) * v + (lambda_v) * value_context\n",
        "\n",
        "        attention_output, _ = self.attention_layer(query = q,\n",
        "                                                   key = k_cap,\n",
        "                                                   value = v_cap)\n",
        "\n",
        "        return attention_output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2ZPn4U3Weq9I"
      },
      "outputs": [],
      "source": [
        "class MAF_acoustic(nn.Module):\n",
        "    def __init__(self,\n",
        "                dim_model,\n",
        "                dropout_rate):\n",
        "        super(MAF_acoustic, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "        # self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "        self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "                                                                dim_context=ACOUSTIC_DIM,\n",
        "                                                                dropout_rate=dropout_rate)\n",
        "\n",
        "        # self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "        #                                                     dim_context=VISUAL_DIM,\n",
        "        #                                                     dropout_rate=dropout_rate)\n",
        "\n",
        "        self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n",
        "        # self.visual_gate = nn.Linear(2*dim_model, dim_model)\n",
        "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "        self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "    def forward(self,\n",
        "                text_input,\n",
        "                acoustic_context):\n",
        "\n",
        "        # print(\"Acoustic context shape (A) : \", acoustic_context.shape)\n",
        "\n",
        "        acoustic_context = acoustic_context.permute(0,2,1)\n",
        "        acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n",
        "        acoustic_context = acoustic_context.permute(0,2,1)\n",
        "\n",
        "        audio_out = self.acoustic_context_attention(q=text_input,\n",
        "                                                    k=text_input,\n",
        "                                                    v=text_input,\n",
        "                                                    context=acoustic_context)\n",
        "        # print(\"Audio out (A) : \", audio_out.shape)\n",
        "\n",
        "        # print(\"Visual context shape : \", visual_context.shape)\n",
        "        # visual_context = visual_context.permute(0,2,1)\n",
        "        # visual_context = self.visual_context_transform(visual_context.float())\n",
        "        # visual_context = visual_context.permute(0,2,1)\n",
        "\n",
        "        # video_out = self.visual_context_attention(q=text_input,\n",
        "        #                                             k=text_input,\n",
        "        #                                             v=text_input,\n",
        "        #                                             context=visual_context)\n",
        "\n",
        "        # print(\"Video out shape : \", video_out.shape)\n",
        "        # print(\"Text input shape : \", text_input.shape)\n",
        "        weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n",
        "        # weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n",
        "\n",
        "        # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n",
        "\n",
        "        output = self.final_layer_norm(text_input + weight_a * audio_out)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZeZdIlcker1V"
      },
      "outputs": [],
      "source": [
        "class MAF_visual(nn.Module):\n",
        "    def __init__(self,\n",
        "                dim_model,\n",
        "                dropout_rate):\n",
        "        super(MAF_visual, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # self.acoustic_context_transform = nn.Linear(ACOUSTIC_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "        self.visual_context_transform = nn.Linear(VISUAL_MAX_LEN, SOURCE_MAX_LEN, bias = False)\n",
        "\n",
        "        # self.acoustic_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "        #                                                         dim_context=ACOUSTIC_DIM,\n",
        "        #                                                         dropout_rate=dropout_rate)\n",
        "\n",
        "        self.visual_context_attention = ContextAwareAttention(dim_model=dim_model,\n",
        "                                                            dim_context=VISUAL_DIM,\n",
        "                                                            dropout_rate=dropout_rate)\n",
        "\n",
        "        # self.acoustic_gate = nn.Linear(2*dim_model, dim_model)\n",
        "        self.visual_gate = nn.Linear(2*dim_model, dim_model)\n",
        "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "        self.final_layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "    def forward(self,\n",
        "                text_input,\n",
        "                visual_context):\n",
        "\n",
        "        # print(\"Acoustic context shape (A) : \", acoustic_context.shape)\n",
        "\n",
        "        # acoustic_context = acoustic_context.permute(0,2,1)\n",
        "        # acoustic_context = self.acoustic_context_transform(acoustic_context.float())\n",
        "        # acoustic_context = acoustic_context.permute(0,2,1)\n",
        "\n",
        "        # audio_out = self.acoustic_context_attention(q=text_input,\n",
        "        #                                             k=text_input,\n",
        "        #                                             v=text_input,\n",
        "        #                                             context=acoustic_context)\n",
        "        # print(\"Audio out (A) : \", audio_out.shape)\n",
        "\n",
        "        # print(\"Visual context shape : \", visual_context.shape)\n",
        "        visual_context = visual_context.permute(0,2,1)\n",
        "        visual_context = self.visual_context_transform(visual_context.float())\n",
        "        visual_context = visual_context.permute(0,2,1)\n",
        "\n",
        "        video_out = self.visual_context_attention(q=text_input,\n",
        "                                                    k=text_input,\n",
        "                                                    v=text_input,\n",
        "                                                    context=visual_context)\n",
        "\n",
        "        # print(\"Video out shape : \", video_out.shape)\n",
        "        # print(\"Text input shape : \", text_input.shape)\n",
        "        # weight_a = F.sigmoid(self.acoustic_gate(torch.cat([text_input, audio_out], dim=-1)))\n",
        "        weight_v = F.sigmoid(self.visual_gate(torch.cat([text_input, video_out], dim=-1)))\n",
        "\n",
        "        # output = self.final_layer_norm(text_input + weight_a * audio_out + weight_v * video_out)\n",
        "\n",
        "        output = self.final_layer_norm(text_input  + weight_v * video_out)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiModalBartEncoder(BartPretrainedModel):\n",
        "\n",
        "    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.encoder_layerdrop\n",
        "\n",
        "        embed_dim = config.d_model\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_source_position = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n",
        "\n",
        "        self.embed_positions = BartLearnedPositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            embed_dim\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.layers = nn.ModuleList([BartEncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "\n",
        "        self.layernorm_embedding = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.init_weights()\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "        # self.fusion_at_layer = [4]\n",
        "        # self.fusion_at_layer = [3, 4]\n",
        "        # self.fusion_at_layer3 = [3]\n",
        "        self.fusion_at_layer4 = [4]\n",
        "        self.fusion_at_layer5 = [5]\n",
        "\n",
        "        # self.fusion_of_context = [3]\n",
        "        # self.visual_transformer = TransformerEncoder(d_model = VISUAL_DIM,\n",
        "        #                                              n_layers = 4,\n",
        "        #                                              n_heads=8,\n",
        "        #                                              d_ff=VISUAL_DIM\n",
        "        #                                              )\n",
        "        # self.acoustic_transformer = TransformerEncoder(d_model = ACOUSTIC_DIM,\n",
        "        #                                                n_layers=4,\n",
        "        #                                                n_heads=2,\n",
        "        #                                                d_ff=ACOUSTIC_DIM)\n",
        "\n",
        "        # self.MAF_layer3 = MAF(dim_model=embed_dim,\n",
        "        #                      dropout_rate=0.2)\n",
        "\n",
        "        self.MAF_layer4 = MAF_acoustic(dim_model=embed_dim,\n",
        "                             dropout_rate=0.2)\n",
        "\n",
        "        self.MAF_layer5 = MAF_visual(dim_model=embed_dim,\n",
        "                             dropout_rate=0.2)\n",
        "\n",
        "        # self.context_encoder = ContextEncoder(config)\n",
        "\n",
        "        # self.classification = nn.Linear(embed_dim, 2)\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "            input_ids = None,\n",
        "            attention_mask = None,\n",
        "            # context_input_ids = None,\n",
        "            # context_attention_mask = None,\n",
        "            acoustic_input = None,\n",
        "            visual_input = None,\n",
        "            head_mask = None,\n",
        "            inputs_embeds = None,\n",
        "            output_attentions = None,\n",
        "            output_hidden_states  = None,\n",
        "            return_dict = None):\n",
        "\n",
        "            # print(\"Input ids shape : \", input_ids.shape)\n",
        "            output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "\n",
        "            output_hidden_states = (\n",
        "                output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "            )\n",
        "\n",
        "            return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "            if input_ids is not None and inputs_embeds is not None:\n",
        "                raise ValueError(\"You can't specify both input_ids and inputs_embeds at the same time\")\n",
        "            elif input_ids is not None:\n",
        "                input_shape = input_ids.size()\n",
        "                input_ids = input_ids.view(-1, input_shape[-1])\n",
        "\n",
        "            elif inputs_embeds is not None:\n",
        "                input_shape = inputs_embeds.size()[:-1]\n",
        "            else:\n",
        "                raise ValueError(\"You have to specify either input_ids or input_embeds\")\n",
        "\n",
        "\n",
        "            if inputs_embeds is None:\n",
        "                # print(\"Input ids shape : \", input_ids.shape)\n",
        "                inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "            # print(\"Input shape type : \", type(input_shape))\n",
        "            # print(\"Input shape : \", input_shape)\n",
        "            input_shape = torch.tensor(input_shape)\n",
        "            # print(\"Input shape type : \", type(input_shape))\n",
        "            # print(\"Input shape : \", input_shape)\n",
        "            # print(\"Input shape : \", input_shape.shape)\n",
        "            embed_pos = self.embed_positions(input_ids)\n",
        "            # embed_pos = self.embed_positions(input_shape)\n",
        "\n",
        "\n",
        "            hidden_states = inputs_embeds + embed_pos\n",
        "            hidden_states = self.layernorm_embedding(hidden_states)\n",
        "            hidden_states = F.dropout(hidden_states, p = self.dropout, training=self.training)\n",
        "\n",
        "            print(\"attention mask shape 3 : \", attention_mask.shape)\n",
        "            if attention_mask is not None:\n",
        "                # attention_mask =  _expand_mask(attention_mask, inputs_embeds.dtype)\n",
        "                batch_size = attention_mask.size(0)\n",
        "                attention_mask = attention_mask.unsqueeze(1).expand(batch_size, 16, -1, -1).to(hidden_states.dtype)\n",
        "                # attention_mask = attention_mask.unsqueeze(1).repeat(32, 16, 1, 1)\n",
        "\n",
        "            print(\"attention mask shape 4 : \", attention_mask.shape)\n",
        "            encoder_states = () if output_hidden_states else None\n",
        "            all_attentions = () if output_attentions else None\n",
        "\n",
        "            if head_mask is not None:\n",
        "                assert head_mask.size()[0] == (\n",
        "                    len(self.layers)\n",
        "                ), f\"The head mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
        "\n",
        "            for idx, encoder_layer in enumerate(self.layers):\n",
        "                # print(\"============Idx : \", idx)\n",
        "\n",
        "                # if idx in self.fusion_at_layer3:\n",
        "                #     # print(\"Acoustic input shape (B) : \", acoustic_input)\n",
        "                #     # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]\n",
        "                #     # print(\"Acoustic input shape (C) : \", acoustic_input)\n",
        "\n",
        "                #     # visual_input = self.visual_transformer(visual_input)[-1]\n",
        "                #     # print(\"====Idx inside fusion at layer :\", idx)\n",
        "                #     hidden_states = self.MAF_layer3(text_input = hidden_states,\n",
        "                #                                    acoustic_context = acoustic_input,\n",
        "                #                                    visual_context = visual_input)\n",
        "\n",
        "                # if idx in self.fusion_of_context:\n",
        "\n",
        "                #   hidden_states = self.context_encoder(hidden_states = hidden_states, context_input_ids = context_input_ids, context_attention_mask = context_attention_mask)\n",
        "\n",
        "\n",
        "                if idx in self.fusion_at_layer4:\n",
        "                    # print(\"Acoustic input shape (B) : \", acoustic_input)\n",
        "                    # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]\n",
        "                    # print(\"Acoustic input shape (C) : \", acoustic_input)\n",
        "\n",
        "                    # visual_input = self.visual_transformer(visual_input)[-1]\n",
        "                    # print(\"====Idx inside fusion at layer :\", idx)\n",
        "\n",
        "                    hidden_states = self.MAF_layer4(text_input = hidden_states,\n",
        "                                                   acoustic_context = acoustic_input\n",
        "                                                   )\n",
        "                if idx in self.fusion_at_layer5:\n",
        "                    # print(\"Acoustic input shape (B) : \", acoustic_input)\n",
        "                    # acoustic_input = self.acoustic_transformer(acoustic_input)[-1]\n",
        "                    # print(\"Acoustic input shape (C) : \", acoustic_input)\n",
        "\n",
        "                    # visual_input = self.visual_transformer(visual_input)[-1]\n",
        "                    # print(\"====Idx inside fusion at layer :\", idx)\n",
        "                    hidden_states = self.MAF_layer5(text_input = hidden_states,\n",
        "                                                   visual_context = visual_input)\n",
        "\n",
        "                if output_hidden_states:\n",
        "                    encoder_states = encoder_states + (hidden_states,)\n",
        "\n",
        "                dropout_probability = random.uniform(0,1)\n",
        "\n",
        "                if self.training and (dropout_probability < self.layerdrop):\n",
        "                    layer_outputs = (None, None)\n",
        "\n",
        "                else:\n",
        "                    if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                        def create_custom_forward(module):\n",
        "                            def custom_forward(*inputs):\n",
        "                                return module(*inputs, output_attentions)\n",
        "\n",
        "                            return custom_forward\n",
        "\n",
        "                        layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                            create_custom_forward(encoder_layer),\n",
        "                            hidden_states,\n",
        "                            attention_mask,\n",
        "                            (head_mask[idx] if head_mask is not None else None),\n",
        "                        )\n",
        "\n",
        "                    else:\n",
        "                        # print(\"Checking Attention mask shape : \", attention_mask.shape)\n",
        "                        layer_outputs = encoder_layer(\n",
        "                            hidden_states,\n",
        "                            attention_mask,\n",
        "                            layer_head_mask = (head_mask[idx] if head_mask is not None else None),\n",
        "                            output_attentions = output_attentions\n",
        "                        )\n",
        "\n",
        "                    hidden_states = layer_outputs[0]\n",
        "\n",
        "                if output_attentions:\n",
        "                    all_attentions  = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "            if output_hidden_states:\n",
        "                encoder_states = encoder_states + (hidden_states,)\n",
        "\n",
        "            if not return_dict:\n",
        "                return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
        "\n",
        "            return BaseModelOutput(\n",
        "                last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
        "            )\n",
        "\n",
        "            # print(\"Hidden states shape : \", hidden_states)\n",
        "\n",
        "            # cls = hidden_states.permute(1,0,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6F_nfly0HgKR"
      },
      "outputs": [],
      "source": [
        "class MultimodalBartModel(BartPretrainedModel):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
        "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
        "\n",
        "        self.encoder = MultiModalBartEncoder(config, self.shared)\n",
        "        self.decoder = BartDecoder(config, self.shared)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.shared = value\n",
        "        self.encoder.embed_tokens = self.shared\n",
        "        self.decoder.embed_tokens = self.shared\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.encoder\n",
        "\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.decoder\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids = None,\n",
        "        attention_mask = None,\n",
        "        # context_input_ids = None,\n",
        "        # context_attention_mask = None,\n",
        "        acoustic_input = None,\n",
        "        visual_input = None,\n",
        "        decoder_input_ids = None,\n",
        "        decoder_attention_mask = None,\n",
        "        head_mask = None,\n",
        "        decoder_head_mask = None,\n",
        "        cross_attn_head_mask = None,\n",
        "        encoder_outputs = None,\n",
        "        past_key_values = None,\n",
        "        inputs_embeds = None,\n",
        "        decoder_inputs_embeds = None,\n",
        "        use_cache = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None\n",
        "    ):\n",
        "\n",
        "        if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
        "            decoder_input_ids = shift_tokens_right(\n",
        "                input_ids, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "\n",
        "            )\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "\n",
        "        # if attention_mask is not None:\n",
        "        #     attention_mask = attention_mask.unsqueeze(1).expand(32, 16, -1, -1).to(dtype=next(self.parameters()).dtype)\n",
        "\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # print(\"attention mask shape 2 : \", attention_mask.shape)\n",
        "\n",
        "        if encoder_outputs is None:\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids = input_ids,\n",
        "                attention_mask = attention_mask,\n",
        "                # context_input_ids = context_input_ids,\n",
        "                # context_attention_mask = context_attention_mask,\n",
        "                acoustic_input = acoustic_input,\n",
        "                visual_input = visual_input,\n",
        "                head_mask = head_mask,\n",
        "                inputs_embeds = inputs_embeds,\n",
        "                output_attentions = output_attentions,\n",
        "                output_hidden_states = output_hidden_states,\n",
        "                return_dict = return_dict\n",
        "            )\n",
        "\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            encoder_outputs = BaseModelOutput(\n",
        "                last_hidden_state=encoder_outputs[0],\n",
        "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "                attentions = encoder_outputs[2] if len(encoder_outputs) > 2 else None\n",
        "            )\n",
        "\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids = decoder_input_ids,\n",
        "            attention_mask = decoder_attention_mask,\n",
        "            encoder_hidden_states = encoder_outputs[0],\n",
        "            encoder_attention_mask = attention_mask,\n",
        "            head_mask = decoder_head_mask,\n",
        "            cross_attn_head_mask = cross_attn_head_mask,\n",
        "            past_key_values = past_key_values,\n",
        "            inputs_embeds = decoder_inputs_embeds,\n",
        "            use_cache = use_cache,\n",
        "            output_attentions = output_attentions,\n",
        "            output_hidden_states = output_hidden_states,\n",
        "            return_dict = return_dict\n",
        "        )\n",
        "\n",
        "        if not return_dict:\n",
        "            return decoder_outputs + encoder_outputs\n",
        "\n",
        "        return Seq2SeqModelOutput(\n",
        "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "            past_key_values=decoder_outputs.past_key_values,\n",
        "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "            decoder_attentions=decoder_outputs.attentions,\n",
        "            cross_attentions=decoder_outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "            encoder_attentions=encoder_outputs.attentions\n",
        "\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VO1KdKseHlkn"
      },
      "outputs": [],
      "source": [
        "class MultimodalBartClassification(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        inned_dim: int,\n",
        "        num_classes: int,\n",
        "        pooler_dropout: float\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, inned_dim)\n",
        "        self.dropout = nn.Dropout(p = pooler_dropout)\n",
        "        self.out_proj = nn.Linear(inned_dim, num_classes)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = torch.tanh(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.out_proj(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultimodalBartForSequenceClassification(BartPretrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n",
        "\n",
        "    def __init__(self, config: BartConfig, **kwargs):\n",
        "        super().__init__(config, **kwargs)\n",
        "        self.model = MultimodalBartModel(config)\n",
        "        self.classification_head = MultimodalBartClassification(\n",
        "            config.d_model,\n",
        "            config.d_model,\n",
        "            2,\n",
        "            config.classifier_dropout\n",
        "        )\n",
        "        self.model._init_weights(self.classification_head.dense)\n",
        "        self.model._init_weights(self.classification_head.out_proj)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.tensor] = None,\n",
        "        # context_input_ids : torch.LongTensor = None,\n",
        "        # context_attention_mask : Optional[torch.tensor] = None,\n",
        "        acoustic_input = None,\n",
        "        visual_input = None,\n",
        "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None\n",
        "    ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "            config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        if labels is not None:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is None and inputs_embeds is not None:\n",
        "            raise NotImplementedError(\n",
        "                f\"Passing input embeddings is currently not supported for {self.__class__.__name__}\"\n",
        "            )\n",
        "\n",
        "        # print(\"attention mask shape 1 : \", attention_mask.shape )\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask = attention_mask,\n",
        "            # context_input_ids = context_input_ids,\n",
        "            # context_attention_mask = context_attention_mask,\n",
        "            acoustic_input = acoustic_input,\n",
        "            visual_input = visual_input,\n",
        "            decoder_input_ids = decoder_input_ids,\n",
        "            decoder_attention_mask = decoder_attention_mask,\n",
        "            head_mask = head_mask,\n",
        "            decoder_head_mask = decoder_head_mask,\n",
        "            cross_attn_head_mask = cross_attn_head_mask,\n",
        "            encoder_outputs = encoder_outputs,\n",
        "            inputs_embeds = inputs_embeds,\n",
        "            decoder_inputs_embeds = decoder_inputs_embeds,\n",
        "            use_cache = use_cache,\n",
        "            output_attentions = output_attentions,\n",
        "            output_hidden_states = output_hidden_states,\n",
        "            return_dict = return_dict\n",
        "\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs[0]\n",
        "\n",
        "        eos_mask = input_ids.eq(self.config.eos_token_id).to(hidden_states.device)\n",
        "\n",
        "        if len(torch.unique_consecutive(eos_mask.sum(1))) > 1:\n",
        "            raise ValueError(\"All examples must have same number of <eos> tokens\")\n",
        "\n",
        "        sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[\n",
        "            :, -1, :\n",
        "        ]\n",
        "\n",
        "        logits = self.classification_head(sentence_representation)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        # print(\"logits shape : \", logits.shape)\n",
        "        loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "        # print(\"Loss : \", loss)\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "\n",
        "        return Seq2SeqSequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
        "            decoder_attentions = outputs.decoder_attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
        "            encoder_attentions=outputs.encoder_attentions\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "FOLDER_NAME = '/backup/hatemm/Dataset/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KaewwlapryS5"
      },
      "outputs": [],
      "source": [
        "with open(FOLDER_NAME + 'all__video_vosk_audioMap.pkl', 'rb') as f:\n",
        "  transcript = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RunFr5b0Ni1u"
      },
      "outputs": [],
      "source": [
        "with open(FOLDER_NAME + 'Wav2Vec2_features_chunked.pkl', 'rb') as fo:\n",
        "  audio_data = pickle.load(fo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Yw0sc0aaNzOO"
      },
      "outputs": [],
      "source": [
        "with open(FOLDER_NAME + 'noFoldDetails.pkl', 'rb') as fp:\n",
        "    video_labels = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qO9elV8EQAi1"
      },
      "outputs": [],
      "source": [
        "def pad_seq(tensor, dim, max_len):\n",
        "  if max_len > tensor.shape[0] :\n",
        "    return torch.cat([tensor, torch.zeros(max_len - tensor.shape[0], dim)])\n",
        "  else:\n",
        "    return tensor[:max_len]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTMNyPhSUdBs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pdf5hGlVH5Zw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of MultimodalBartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'encoder.MAF_layer4.acoustic_context_attention.attention_layer.in_proj_bias', 'encoder.MAF_layer4.acoustic_context_attention.attention_layer.in_proj_weight', 'encoder.MAF_layer4.acoustic_context_attention.attention_layer.out_proj.bias', 'encoder.MAF_layer4.acoustic_context_attention.attention_layer.out_proj.weight', 'encoder.MAF_layer4.acoustic_context_attention.u_k.weight', 'encoder.MAF_layer4.acoustic_context_attention.u_v.weight', 'encoder.MAF_layer4.acoustic_context_attention.w1_k.weight', 'encoder.MAF_layer4.acoustic_context_attention.w1_v.weight', 'encoder.MAF_layer4.acoustic_context_attention.w2_k.weight', 'encoder.MAF_layer4.acoustic_context_attention.w2_v.weight', 'encoder.MAF_layer4.acoustic_context_transform.weight', 'encoder.MAF_layer4.acoustic_gate.bias', 'encoder.MAF_layer4.acoustic_gate.weight', 'encoder.MAF_layer4.final_layer_norm.bias', 'encoder.MAF_layer4.final_layer_norm.weight', 'encoder.MAF_layer5.final_layer_norm.bias', 'encoder.MAF_layer5.final_layer_norm.weight', 'encoder.MAF_layer5.visual_context_attention.attention_layer.in_proj_bias', 'encoder.MAF_layer5.visual_context_attention.attention_layer.in_proj_weight', 'encoder.MAF_layer5.visual_context_attention.attention_layer.out_proj.bias', 'encoder.MAF_layer5.visual_context_attention.attention_layer.out_proj.weight', 'encoder.MAF_layer5.visual_context_attention.u_k.weight', 'encoder.MAF_layer5.visual_context_attention.u_v.weight', 'encoder.MAF_layer5.visual_context_attention.w1_k.weight', 'encoder.MAF_layer5.visual_context_attention.w1_v.weight', 'encoder.MAF_layer5.visual_context_attention.w2_k.weight', 'encoder.MAF_layer5.visual_context_attention.w2_v.weight', 'encoder.MAF_layer5.visual_context_transform.weight', 'encoder.MAF_layer5.visual_gate.bias', 'encoder.MAF_layer5.visual_gate.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultimodalBartForSequenceClassification(\n",
            "  (model): MultimodalBartModel(\n",
            "    (shared): Embedding(50265, 768, padding_idx=1)\n",
            "    (encoder): MultiModalBartEncoder(\n",
            "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
            "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
            "      (layers): ModuleList(\n",
            "        (0-5): 6 x BartEncoderLayer(\n",
            "          (self_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (activation_fn): GELUActivation()\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (MAF_layer4): MAF_acoustic(\n",
            "        (acoustic_context_transform): Linear(in_features=1000, out_features=768, bias=False)\n",
            "        (acoustic_context_attention): ContextAwareAttention(\n",
            "          (attention_layer): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (u_k): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (w1_k): Linear(in_features=768, out_features=1, bias=False)\n",
            "          (w2_k): Linear(in_features=768, out_features=1, bias=False)\n",
            "          (u_v): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (w1_v): Linear(in_features=768, out_features=1, bias=False)\n",
            "          (w2_v): Linear(in_features=768, out_features=1, bias=False)\n",
            "        )\n",
            "        (acoustic_gate): Linear(in_features=1536, out_features=768, bias=True)\n",
            "        (dropout_layer): Dropout(p=0.2, inplace=False)\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (MAF_layer5): MAF_visual(\n",
            "        (visual_context_transform): Linear(in_features=1000, out_features=768, bias=False)\n",
            "        (visual_context_attention): ContextAwareAttention(\n",
            "          (attention_layer): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (u_k): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (w1_k): Linear(in_features=768, out_features=1, bias=False)\n",
            "          (w2_k): Linear(in_features=768, out_features=1, bias=False)\n",
            "          (u_v): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (w1_v): Linear(in_features=768, out_features=1, bias=False)\n",
            "          (w2_v): Linear(in_features=768, out_features=1, bias=False)\n",
            "        )\n",
            "        (visual_gate): Linear(in_features=1536, out_features=768, bias=True)\n",
            "        (dropout_layer): Dropout(p=0.2, inplace=False)\n",
            "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (decoder): BartDecoder(\n",
            "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
            "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
            "      (layers): ModuleList(\n",
            "        (0-5): 6 x BartDecoderLayer(\n",
            "          (self_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (activation_fn): GELUActivation()\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_head): MultimodalBartClassification(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "Tokenizer :  BartTokenizerFast(name_or_path='facebook/bart-base', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
            "}\n",
            "Total parameters :  151.002626\n"
          ]
        }
      ],
      "source": [
        "model = MultimodalBartForSequenceClassification.from_pretrained(\"facebook/bart-base\")\n",
        "print(model)\n",
        "\n",
        "tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-base')\n",
        "print(\"Tokenizer : \", tokenizer)\n",
        "\n",
        "num_param = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters : \", num_param/1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "X0y5DYklxuUT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BartTokenizerFast(name_or_path='facebook/bart-base', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['[UTTERANCE]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
              "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
              "\t50265: AddedToken(\"[UTTERANCE]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p = {\n",
        "        # 'additional_special_tokens' : ['[CONTEXT]', '[UTTERANCE]']\n",
        "        'additional_special_tokens' : ['[UTTERANCE]']\n",
        "    }\n",
        "\n",
        "tokenizer.add_special_tokens(p)\n",
        "\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "P1Ctmwokx-T2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(50266, 768)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "C2dU7MfiZaDq"
      },
      "outputs": [],
      "source": [
        "class HateMMDataset(data.Dataset):\n",
        "    \"Characterizes a dataset for PyTorch\"\n",
        "    def __init__(self, folders, labels):\n",
        "        \"Initialization\"\n",
        "        # print(folders, labels)\n",
        "        self.labels = labels\n",
        "        self.folders = folders\n",
        "\n",
        "    def __len__(self):\n",
        "        \"Denotes the total number of samples\"\n",
        "        return len(self.folders)\n",
        "\n",
        "    def load_data_for_video(self, video):\n",
        "        video_file_name_without_extension, _ = os.path.splitext(video)\n",
        "        pickle_file_path = os.path.join(FOLDER_NAME, \"VITF_new\", video_file_name_without_extension + \"_vit.p\")\n",
        "        \n",
        "        # Load text data\n",
        "        if video in transcript:\n",
        "            # text_features = torch.tensor(np.array(transcript[video]), dtype=torch.float32)\n",
        "            text_features = tokenizer(transcript[video], max_length = SOURCE_MAX_LEN, padding = 'max_length', truncation = True)\n",
        "        else:\n",
        "            raise ValueError(f\"Text data not found for {video}\")\n",
        "        \n",
        "        # Load video data\n",
        "        try:\n",
        "            with open(pickle_file_path, 'rb') as fp:\n",
        "                video_data = pickle.load(fp)\n",
        "                video_features = torch.tensor(np.array(list(video_data.values())), dtype=torch.float32)\n",
        "                # video_features = torch.tensor(np.array(list(video_data.values().mean(dim=0))), dtype=torch.float32)\n",
        "                video_features = video_features.mean(dim=0)\n",
        "        except FileNotFoundError:\n",
        "            raise ValueError(f\"Video data file not found: {pickle_file_path}\")\n",
        "        \n",
        "        # Load audio data\n",
        "        if video in audio_data:\n",
        "            audio_features = torch.tensor(np.array(audio_data[video]), dtype=torch.float32)\n",
        "            audio_features = audio_features.mean(dim=0)\n",
        "            # audio_features, _ = audio_features.max(dim=0)\n",
        "        else:\n",
        "            raise ValueError(f\"Audio data not found for {video}\")\n",
        "        \n",
        "        return text_features, video_features, audio_features\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"Generates one sample of data\"\n",
        "        try:\n",
        "            # Select sample\n",
        "            folder = self.folders[index]\n",
        "            # Load data\n",
        "            X_text, X_vid, X_audio = self.load_data_for_video(folder)\n",
        "            y = torch.LongTensor([self.labels[index]]) \n",
        "            \n",
        "            # return X_text, X_vid, X_audio, y\n",
        "            return torch.tensor(X_text['input_ids'], dtype=torch.long), torch.tensor(X_text['attention_mask'], dtype=torch.bool), X_audio, X_vid, y\n",
        "        \n",
        "        except Exception as e:\n",
        "            # traceback.print_exc()\n",
        "            print(f\"Error loading data for index {index}: {e}\")                        \n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_train_data, all_train_label = video_labels['train']\n",
        "all_val_data, all_val_label = video_labels['val']\n",
        "all_test_data, all_test_label = video_labels['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    batch = list(filter(lambda x: x is not None, batch))\n",
        "    if len(batch) == 0:  # Check if the batch is empty after filtering\n",
        "        return None\n",
        "\n",
        "    return torch.utils.data.dataloader.default_collate(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# training parameters\n",
        "k = 2            # number of target category\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "log_interval = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 2, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
        "valParams = {'batch_size': batch_size, 'shuffle': False, 'num_workers': 2, 'pin_memory': True} if torch.cuda.is_available() else {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "M7uPY9VkLRcr"
      },
      "outputs": [],
      "source": [
        "train_set, val_set, test_set = HateMMDataset(all_train_data, all_train_label), HateMMDataset(all_val_data, all_val_label), HateMMDataset(all_test_data, all_test_label)\n",
        "train_loader = data.DataLoader(train_set, collate_fn = collate_fn, **params)\n",
        "test_loader = data.DataLoader(test_set, collate_fn = collate_fn, **valParams)\n",
        "valid_loader = data.DataLoader(val_set, collate_fn = collate_fn, **valParams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input IDs shape: torch.Size([768])\n",
            "Attention Mask shape: torch.Size([768])\n",
            "Audio shape: torch.Size([768])\n",
            "Video shape: torch.Size([100, 768])\n"
          ]
        }
      ],
      "source": [
        "print(\"Input IDs shape:\", train_set[0][0].shape)\n",
        "print(\"Attention Mask shape:\", train_set[0][1].shape)\n",
        "print(\"Audio shape:\", train_set[0][2].shape)\n",
        "print(\"Video shape:\", train_set[0][3].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1wRtyIFYS2m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MultimodalBartForSequenceClassification(\n",
              "  (model): MultimodalBartModel(\n",
              "    (shared): Embedding(50266, 768)\n",
              "    (encoder): MultiModalBartEncoder(\n",
              "      (embed_tokens): Embedding(50266, 768)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (MAF_layer4): MAF_acoustic(\n",
              "        (acoustic_context_transform): Linear(in_features=1000, out_features=768, bias=False)\n",
              "        (acoustic_context_attention): ContextAwareAttention(\n",
              "          (attention_layer): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (u_k): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (w1_k): Linear(in_features=768, out_features=1, bias=False)\n",
              "          (w2_k): Linear(in_features=768, out_features=1, bias=False)\n",
              "          (u_v): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (w1_v): Linear(in_features=768, out_features=1, bias=False)\n",
              "          (w2_v): Linear(in_features=768, out_features=1, bias=False)\n",
              "        )\n",
              "        (acoustic_gate): Linear(in_features=1536, out_features=768, bias=True)\n",
              "        (dropout_layer): Dropout(p=0.2, inplace=False)\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (MAF_layer5): MAF_visual(\n",
              "        (visual_context_transform): Linear(in_features=1000, out_features=768, bias=False)\n",
              "        (visual_context_attention): ContextAwareAttention(\n",
              "          (attention_layer): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (u_k): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (w1_k): Linear(in_features=768, out_features=1, bias=False)\n",
              "          (w2_k): Linear(in_features=768, out_features=1, bias=False)\n",
              "          (u_v): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (w1_v): Linear(in_features=768, out_features=1, bias=False)\n",
              "          (w2_v): Linear(in_features=768, out_features=1, bias=False)\n",
              "        )\n",
              "        (visual_gate): Linear(in_features=1536, out_features=768, bias=True)\n",
              "        (dropout_layer): Dropout(p=0.2, inplace=False)\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(50266, 768)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (classification_head): MultimodalBartClassification(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 2 GPUs!\n"
          ]
        }
      ],
      "source": [
        "# Parallelize model to multiple GPUs\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "i5MxPWWyIotE"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, data_loader):\n",
        "      model.train()\n",
        "      epoch_train_loss = 0.0\n",
        "\n",
        "\n",
        "      for step, batch in enumerate(tqdm(data_loader, desc = 'Training Iteration')):\n",
        "        # for i, t in enumerate(batch):\n",
        "        #     print(\"Inside hello\")\n",
        "        #     print(i, \" : \", type(t))\n",
        "        # batch = tuple(t.to(DEVICE) for t in batch)\n",
        "        input_ids, attention_mask, acoustic_input, visual_input, labels = batch\n",
        "        print(\"Input IDs shape:\", input_ids.shape)\n",
        "        print(\"Attention Mask shape:\", attention_mask.shape)\n",
        "        print(\"Audio shape:\", acoustic_input.shape)\n",
        "        print(\"Video shape:\", visual_input.shape)\n",
        "        print(\"Labels shape:\", labels.shape)\n",
        "\n",
        "        input_ids, attention_mask, acoustic_input, visual_input, labels = input_ids.to(DEVICE), attention_mask.to(DEVICE), acoustic_input.to(DEVICE), visual_input.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        # print(\"Input ids shape : \", input_ids.shape)\n",
        "        # print(\"Input ids shape : \", input_ids.shape)\n",
        "        outputs = model(input_ids = input_ids,\n",
        "                        attention_mask = attention_mask,\n",
        "                        # context_input_ids = context_input_ids,\n",
        "                        # context_attention_mask = context_attention_mask,\n",
        "                        acoustic_input = acoustic_input,\n",
        "                        visual_input = visual_input,\n",
        "                        labels = labels)\n",
        "\n",
        "        loss = outputs['loss']\n",
        "        epoch_train_loss += loss.item()\n",
        "\n",
        "        # print(\"Batch wise loss : \", epoch_train_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "      print(\"Epoch train loss : \", epoch_train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "D0H6Hk2w62xu"
      },
      "outputs": [],
      "source": [
        "def valid_epoch(model, data_loader):\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  gold = []\n",
        "\n",
        "  valid_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(tqdm(data_loader)):\n",
        "      # batch = tuple(t.to(DEVICE) for t in batch)\n",
        "      input_ids, attention_mask, acoustic_input, visual_input, labels = batch\n",
        "      input_ids, attention_mask, acoustic_input, visual_input, labels = input_ids.to(DEVICE), attention_mask.to(DEVICE), acoustic_input.to(DEVICE), visual_input.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "      outputs = model(input_ids = input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            # context_input_ids = context_input_ids,\n",
        "                            # context_attention_mask = context_attention_mask,\n",
        "                            acoustic_input = acoustic_input,\n",
        "                            visual_input = visual_input,\n",
        "                            labels = labels)\n",
        "\n",
        "      logits = outputs['logits']\n",
        "      loss = outputs['loss']\n",
        "\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "      pred = logits.argmax(dim = -1)\n",
        "\n",
        "      predictions.extend(pred.tolist())\n",
        "      gold.extend(labels.tolist())\n",
        "\n",
        "  return valid_loss, predictions, gold\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "93nGID9-Imr-"
      },
      "outputs": [],
      "source": [
        "def test_epoch(model, data_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    gold = []\n",
        "\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(tqdm(data_loader)):\n",
        "            # batch = tuple(t.to(DEVICE) for t in batch)\n",
        "            input_ids, attention_mask, acoustic_input, visual_input, labels = batch\n",
        "            input_ids, attention_mask, acoustic_input, visual_input, labels = input_ids.to(DEVICE), attention_mask.to(DEVICE), acoustic_input.to(DEVICE), visual_input.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            outputs = model(input_ids = input_ids,\n",
        "                            attention_mask = attention_mask,\n",
        "                            # context_input_ids = context_input_ids,\n",
        "                            # context_attention_mask = context_attention_mask,\n",
        "\n",
        "                            acoustic_input = acoustic_input,\n",
        "                            visual_input = visual_input,\n",
        "                            labels = labels)\n",
        "\n",
        "            logits = outputs['logits']\n",
        "\n",
        "            pred = logits.argmax(dim = -1)\n",
        "\n",
        "            predictions.extend(pred.tolist())\n",
        "\n",
        "            gold.extend(labels.tolist())\n",
        "\n",
        "            correct += int((pred == labels).sum())\n",
        "\n",
        "    return correct/len(data_loader.dataset), predictions, gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "s5QauBgKBtYS"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "  def __init__(self, patience, min_delta):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.counter = 0\n",
        "    self.min_validation = np.inf\n",
        "\n",
        "  def early_stop(self, valid_loss):\n",
        "    if valid_loss < self.min_validation:\n",
        "      self.min_validation = valid_loss\n",
        "      self.counter = 0\n",
        "    elif valid_loss > (self.min_validation + self.min_delta):\n",
        "      self.counter += 1\n",
        "      if self.counter >= self.patience:\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8dHmyfM2Cc9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.EarlyStopping at 0x7fdb84136fb0>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "early_stopper = EarlyStopping(patience = 15, min_delta = 0.2)\n",
        "early_stopper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "B0HB4PIc6ixU"
      },
      "outputs": [],
      "source": [
        "def train_and_validation(model, train_loader, valid_loader):\n",
        "  # lowest_loss = 1e6\n",
        "  best_f1 = 0.0\n",
        "  # min_loss = 1e6\n",
        "  for epoch in range(3):\n",
        "    print(\"\\n=============Epoch : \", epoch)\n",
        "    train_epoch(model, train_loader)\n",
        "    valid_loss, valid_pred, valid_gold = valid_epoch(model, valid_loader)\n",
        "\n",
        "    if early_stopper.early_stop(valid_loss):\n",
        "      break\n",
        "\n",
        "    print(\"Length of predictions : \", len(valid_pred))\n",
        "    print(\"Length of gold : \", len(valid_gold))\n",
        "    print(\"Valid loss : \", valid_loss)\n",
        "    print(\"\\n Valid Accuracy : \", accuracy_score(valid_gold, valid_pred))\n",
        "    print(\"\\n Valid Precision : \", precision_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "    print(\"\\n Valid Recall : \", recall_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "    print(\"\\nValid F1 score : \", f1_score(valid_gold, valid_pred, average = 'weighted'))\n",
        "\n",
        "\n",
        "    curr_f1 = f1_score(valid_gold, valid_pred, average = 'weighted')\n",
        "\n",
        "    curr_loss = valid_loss\n",
        "    # if((curr_f1 > best_f1) and (epoch>=4)):\n",
        "    if(curr_f1 > best_f1):\n",
        "    # if(curr_loss < min_loss):\n",
        "    # if(curr_loss < lowest_loss):\n",
        "      best_f1 = curr_f1\n",
        "      # min_loss = curr_loss\n",
        "      # print(\"Valid pred : \", valid_pred)\n",
        "      # print('valid_gold : ', valid_gold)\n",
        "      # torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/32/saved_model/best_case/best_model_epoch_'+str(epoch)+'_best_f1_'+str(int(best_f1*100))+'_foldNum_'+str(foldNum)+'.pt')\n",
        "      # print(\"model saved\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "RMJL4HbkEBh2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=============Epoch :  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Iteration:   0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error loading data for index 365: Video data file not found: /backup/hatemm/Dataset/VITF_new/hate_video_292_vit.p\n",
            "Error loading data for index 645: Audio data not found for non_hate_video_42.mp4\n",
            "Error loading data for index 232: Text data not found for non_hate_video_252.mp4\n",
            "Input IDs shape: torch.Size([31, 768])\n",
            "Attention Mask shape: torch.Size([31, 768])\n",
            "Audio shape: torch.Size([31, 768])\n",
            "Video shape: torch.Size([31, 100, 768])\n",
            "Labels shape: torch.Size([31, 1])\n",
            "Error loading data for index 516: Text data not found for non_hate_video_262.mp4\n",
            "attention mask shape 3 :  torch.Size([16, 768])\n",
            "attention mask shape 3 :  torch.Size([15, 768])\n",
            "attention mask shape 4 :  torch.Size([16, 16, 1, 768])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Iteration:   0%|          | 0/25 [00:03<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_2357646/3671525936.py\", line 78, in forward\n    outputs = self.model(\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_2357646/3529742881.py\", line 71, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_2357646/811790391.py\", line 199, in forward\n    layer_outputs = encoder_layer(\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\", line 662, in forward\n    hidden_states, attn_weights, _ = self.self_attn(\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\", line 590, in forward\n    attn_output = torch.nn.functional.scaled_dot_product_attention(\nRuntimeError: The expanded size of the tensor (12) must match the existing size (16) at non-singleton dimension 1.  Target sizes: [16, 12, 768, 768].  Tensor sizes: [16, 16, 1, 768]\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train_and_validation(model, train_loader, test_loader)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_and_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[35], line 7\u001b[0m, in \u001b[0;36mtrain_and_validation\u001b[0;34m(model, train_loader, valid_loader)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=============Epoch : \u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch)\n\u001b[0;32m----> 7\u001b[0m   \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m   valid_loss, valid_pred, valid_gold \u001b[38;5;241m=\u001b[39m valid_epoch(model, valid_loader)\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m early_stopper\u001b[38;5;241m.\u001b[39mearly_stop(valid_loss):\n",
            "Cell \u001b[0;32mIn[30], line 22\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print(\"Input ids shape : \", input_ids.shape)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(\"Input ids shape : \", input_ids.shape)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# context_input_ids = context_input_ids,\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# context_attention_mask = context_attention_mask,\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m                \u001b[49m\u001b[43macoustic_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43macoustic_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvisual_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvisual_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     31\u001b[0m epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
            "File \u001b[0;32m~/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:110\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    108\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 110\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m~/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_2357646/3671525936.py\", line 78, in forward\n    outputs = self.model(\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_2357646/3529742881.py\", line 71, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_2357646/811790391.py\", line 199, in forward\n    layer_outputs = encoder_layer(\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\", line 662, in forward\n    hidden_states, attn_weights, _ = self.self_attn(\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/diptesh/anaconda3/envs/hatemm/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\", line 590, in forward\n    attn_output = torch.nn.functional.scaled_dot_product_attention(\nRuntimeError: The expanded size of the tensor (12) must match the existing size (16) at non-singleton dimension 1.  Target sizes: [16, 12, 768, 768].  Tensor sizes: [16, 16, 1, 768]\n"
          ]
        }
      ],
      "source": [
        "# train_and_validation(model, train_loader, test_loader)\n",
        "train_and_validation(model, train_loader, valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCxGBr5x6jcE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgEpzAt-_mSQ"
      },
      "outputs": [],
      "source": [
        "acc, test_pred, test_gold = test_epoch(model, test_loader)\n",
        "\n",
        "print(acc)\n",
        "\n",
        "print(\"\\nAccuracy : \", accuracy_score(test_gold, test_pred))\n",
        "print(\"\\nPrecision : \", precision_score(test_gold, test_pred, average = 'weighted'))\n",
        "print(\"\\nRecall : \", recall_score(test_gold, test_pred, average = 'weighted'))\n",
        "print(\"\\nF1 score : \", f1_score(test_gold, test_pred, average = 'weighted'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('\\nConfusion Matrix : ', confusion_matrix(test_gold, test_pred))\n",
        "\n",
        "sar = 0\n",
        "\n",
        "for j in range(len(test_pred)):\n",
        "  if((test_pred[j]==1) and (test_gold[j] == 1)):\n",
        "    sar+=1\n",
        "\n",
        "\n",
        "print(\"Sarcastic count : \", sar)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eD8Mc-spvSz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuClass": "premium",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
